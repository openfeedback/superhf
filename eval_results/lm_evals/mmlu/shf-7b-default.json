{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542127,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "hendrycksTest-anatomy": {
      "acc": 0.3925925925925926,
      "acc_stderr": 0.04218506215368879,
      "acc_norm": 0.28888888888888886,
      "acc_norm_stderr": 0.0391545063041425
    },
    "hendrycksTest-astronomy": {
      "acc": 0.34210526315789475,
      "acc_stderr": 0.03860731599316091,
      "acc_norm": 0.42105263157894735,
      "acc_norm_stderr": 0.04017901275981748
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.53,
      "acc_stderr": 0.050161355804659205,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.049604496374885836
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.3660377358490566,
      "acc_stderr": 0.02964781353936525,
      "acc_norm": 0.37358490566037733,
      "acc_norm_stderr": 0.029773082713319878
    },
    "hendrycksTest-college_biology": {
      "acc": 0.2986111111111111,
      "acc_stderr": 0.03827052357950756,
      "acc_norm": 0.3055555555555556,
      "acc_norm_stderr": 0.03852084696008534
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.25,
      "acc_stderr": 0.04351941398892446,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.045126085985421276
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.27,
      "acc_stderr": 0.04461960433384741,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.25,
      "acc_stderr": 0.04351941398892446,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.04408440022768078
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.35260115606936415,
      "acc_stderr": 0.03643037168958548,
      "acc_norm": 0.31213872832369943,
      "acc_norm_stderr": 0.03533133389323657
    },
    "hendrycksTest-college_physics": {
      "acc": 0.2549019607843137,
      "acc_stderr": 0.043364327079931785,
      "acc_norm": 0.3235294117647059,
      "acc_norm_stderr": 0.046550104113196177
    },
    "hendrycksTest-computer_security": {
      "acc": 0.41,
      "acc_stderr": 0.04943110704237101,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.049604496374885836
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.3404255319148936,
      "acc_stderr": 0.03097669299853443,
      "acc_norm": 0.22127659574468084,
      "acc_norm_stderr": 0.02713634960242406
    },
    "hendrycksTest-econometrics": {
      "acc": 0.30701754385964913,
      "acc_stderr": 0.043391383225798615,
      "acc_norm": 0.17543859649122806,
      "acc_norm_stderr": 0.0357795481394837
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.35172413793103446,
      "acc_stderr": 0.0397923663749741,
      "acc_norm": 0.31724137931034485,
      "acc_norm_stderr": 0.038783523721386215
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.291005291005291,
      "acc_stderr": 0.023393826500484865,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.023068188848261104
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.3888888888888889,
      "acc_stderr": 0.04360314860077459,
      "acc_norm": 0.3253968253968254,
      "acc_norm_stderr": 0.04190596438871136
    },
    "hendrycksTest-global_facts": {
      "acc": 0.27,
      "acc_stderr": 0.0446196043338474,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542127
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.3709677419354839,
      "acc_stderr": 0.027480541887953593,
      "acc_norm": 0.3387096774193548,
      "acc_norm_stderr": 0.026923446059302844
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.22660098522167488,
      "acc_stderr": 0.029454863835292996,
      "acc_norm": 0.3054187192118227,
      "acc_norm_stderr": 0.03240661565868408
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.38,
      "acc_stderr": 0.04878317312145632,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.048241815132442176
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.40606060606060607,
      "acc_stderr": 0.03834816355401181,
      "acc_norm": 0.38181818181818183,
      "acc_norm_stderr": 0.037937131711656344
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.35858585858585856,
      "acc_stderr": 0.03416903640391521,
      "acc_norm": 0.36363636363636365,
      "acc_norm_stderr": 0.03427308652999934
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.41968911917098445,
      "acc_stderr": 0.035615873276858834,
      "acc_norm": 0.3626943005181347,
      "acc_norm_stderr": 0.03469713791704371
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.3435897435897436,
      "acc_stderr": 0.02407869658063547,
      "acc_norm": 0.2948717948717949,
      "acc_norm_stderr": 0.02311936275823228
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.22962962962962963,
      "acc_stderr": 0.02564410863926762,
      "acc_norm": 0.29259259259259257,
      "acc_norm_stderr": 0.02773896963217609
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.35714285714285715,
      "acc_stderr": 0.031124619309328177,
      "acc_norm": 0.3487394957983193,
      "acc_norm_stderr": 0.030956636328566548
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2052980132450331,
      "acc_stderr": 0.03297986648473836,
      "acc_norm": 0.2185430463576159,
      "acc_norm_stderr": 0.033742355504256936
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.43853211009174314,
      "acc_stderr": 0.021274713073954565,
      "acc_norm": 0.3339449541284404,
      "acc_norm_stderr": 0.020220554196736403
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.3194444444444444,
      "acc_stderr": 0.0317987634217685,
      "acc_norm": 0.3148148148148148,
      "acc_norm_stderr": 0.03167468706828978
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.4362745098039216,
      "acc_stderr": 0.034806931384570375,
      "acc_norm": 0.4019607843137255,
      "acc_norm_stderr": 0.034411900234824655
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.45569620253164556,
      "acc_stderr": 0.03241920684693334,
      "acc_norm": 0.4177215189873418,
      "acc_norm_stderr": 0.032103530322412685
    },
    "hendrycksTest-human_aging": {
      "acc": 0.42152466367713004,
      "acc_stderr": 0.03314190222110658,
      "acc_norm": 0.29596412556053814,
      "acc_norm_stderr": 0.030636591348699813
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.4732824427480916,
      "acc_stderr": 0.04379024936553894,
      "acc_norm": 0.3816793893129771,
      "acc_norm_stderr": 0.0426073515764456
    },
    "hendrycksTest-international_law": {
      "acc": 0.4297520661157025,
      "acc_stderr": 0.04519082021319774,
      "acc_norm": 0.5454545454545454,
      "acc_norm_stderr": 0.04545454545454548
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.42592592592592593,
      "acc_stderr": 0.047803436269367894,
      "acc_norm": 0.5185185185185185,
      "acc_norm_stderr": 0.04830366024635331
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.32515337423312884,
      "acc_stderr": 0.036803503712864595,
      "acc_norm": 0.3619631901840491,
      "acc_norm_stderr": 0.037757007291414416
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.36607142857142855,
      "acc_stderr": 0.0457237235873743,
      "acc_norm": 0.38392857142857145,
      "acc_norm_stderr": 0.04616143075028547
    },
    "hendrycksTest-management": {
      "acc": 0.4174757281553398,
      "acc_stderr": 0.04882840548212238,
      "acc_norm": 0.34951456310679613,
      "acc_norm_stderr": 0.04721188506097171
    },
    "hendrycksTest-marketing": {
      "acc": 0.6068376068376068,
      "acc_stderr": 0.03199957924651047,
      "acc_norm": 0.5170940170940171,
      "acc_norm_stderr": 0.032736940493481824
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.38,
      "acc_stderr": 0.04878317312145633,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.047258156262526045
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.5376756066411239,
      "acc_stderr": 0.017829131764287177,
      "acc_norm": 0.3959131545338442,
      "acc_norm_stderr": 0.017488247006979273
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.34971098265895956,
      "acc_stderr": 0.025674281456531018,
      "acc_norm": 0.3063583815028902,
      "acc_norm_stderr": 0.02481835012943659
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.23798882681564246,
      "acc_stderr": 0.014242630070574915,
      "acc_norm": 0.30502793296089387,
      "acc_norm_stderr": 0.015398723510916713
    },
    "hendrycksTest-nutrition": {
      "acc": 0.38235294117647056,
      "acc_stderr": 0.02782610930728369,
      "acc_norm": 0.4117647058823529,
      "acc_norm_stderr": 0.02818059632825929
    },
    "hendrycksTest-philosophy": {
      "acc": 0.34726688102893893,
      "acc_stderr": 0.027040745502307336,
      "acc_norm": 0.3247588424437299,
      "acc_norm_stderr": 0.026596782287697046
    },
    "hendrycksTest-prehistory": {
      "acc": 0.4228395061728395,
      "acc_stderr": 0.027487472980871595,
      "acc_norm": 0.32407407407407407,
      "acc_norm_stderr": 0.026041766202717163
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.2978723404255319,
      "acc_stderr": 0.027281608344469414,
      "acc_norm": 0.2553191489361702,
      "acc_norm_stderr": 0.02601199293090201
    },
    "hendrycksTest-professional_law": {
      "acc": 0.2894393741851369,
      "acc_stderr": 0.011582659702210249,
      "acc_norm": 0.3070404172099087,
      "acc_norm_stderr": 0.011780959114513769
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.2867647058823529,
      "acc_stderr": 0.027472274473233818,
      "acc_norm": 0.2867647058823529,
      "acc_norm_stderr": 0.027472274473233818
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.3349673202614379,
      "acc_stderr": 0.0190942281670003,
      "acc_norm": 0.3137254901960784,
      "acc_norm_stderr": 0.018771683893528186
    },
    "hendrycksTest-public_relations": {
      "acc": 0.44545454545454544,
      "acc_stderr": 0.047605488214603246,
      "acc_norm": 0.2636363636363636,
      "acc_norm_stderr": 0.04220224692971987
    },
    "hendrycksTest-security_studies": {
      "acc": 0.4122448979591837,
      "acc_stderr": 0.0315123604467428,
      "acc_norm": 0.3183673469387755,
      "acc_norm_stderr": 0.029822533793982055
    },
    "hendrycksTest-sociology": {
      "acc": 0.472636815920398,
      "acc_stderr": 0.035302355173346824,
      "acc_norm": 0.3781094527363184,
      "acc_norm_stderr": 0.03428867848778658
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.56,
      "acc_stderr": 0.049888765156985884,
      "acc_norm": 0.52,
      "acc_norm_stderr": 0.050211673156867795
    },
    "hendrycksTest-virology": {
      "acc": 0.3433734939759036,
      "acc_stderr": 0.03696584317010601,
      "acc_norm": 0.3493975903614458,
      "acc_norm_stderr": 0.037117251907407486
    },
    "hendrycksTest-world_religions": {
      "acc": 0.631578947368421,
      "acc_stderr": 0.036996580176568775,
      "acc_norm": 0.5614035087719298,
      "acc_norm_stderr": 0.038057975055904594
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 0,
    "hendrycksTest-anatomy": 0,
    "hendrycksTest-astronomy": 0,
    "hendrycksTest-business_ethics": 0,
    "hendrycksTest-clinical_knowledge": 0,
    "hendrycksTest-college_biology": 0,
    "hendrycksTest-college_chemistry": 0,
    "hendrycksTest-college_computer_science": 0,
    "hendrycksTest-college_mathematics": 0,
    "hendrycksTest-college_medicine": 0,
    "hendrycksTest-college_physics": 0,
    "hendrycksTest-computer_security": 0,
    "hendrycksTest-conceptual_physics": 0,
    "hendrycksTest-econometrics": 0,
    "hendrycksTest-electrical_engineering": 0,
    "hendrycksTest-elementary_mathematics": 0,
    "hendrycksTest-formal_logic": 0,
    "hendrycksTest-global_facts": 0,
    "hendrycksTest-high_school_biology": 0,
    "hendrycksTest-high_school_chemistry": 0,
    "hendrycksTest-high_school_computer_science": 0,
    "hendrycksTest-high_school_european_history": 0,
    "hendrycksTest-high_school_geography": 0,
    "hendrycksTest-high_school_government_and_politics": 0,
    "hendrycksTest-high_school_macroeconomics": 0,
    "hendrycksTest-high_school_mathematics": 0,
    "hendrycksTest-high_school_microeconomics": 0,
    "hendrycksTest-high_school_physics": 0,
    "hendrycksTest-high_school_psychology": 0,
    "hendrycksTest-high_school_statistics": 0,
    "hendrycksTest-high_school_us_history": 0,
    "hendrycksTest-high_school_world_history": 0,
    "hendrycksTest-human_aging": 0,
    "hendrycksTest-human_sexuality": 0,
    "hendrycksTest-international_law": 0,
    "hendrycksTest-jurisprudence": 0,
    "hendrycksTest-logical_fallacies": 0,
    "hendrycksTest-machine_learning": 0,
    "hendrycksTest-management": 0,
    "hendrycksTest-marketing": 0,
    "hendrycksTest-medical_genetics": 0,
    "hendrycksTest-miscellaneous": 0,
    "hendrycksTest-moral_disputes": 0,
    "hendrycksTest-moral_scenarios": 0,
    "hendrycksTest-nutrition": 0,
    "hendrycksTest-philosophy": 0,
    "hendrycksTest-prehistory": 0,
    "hendrycksTest-professional_accounting": 0,
    "hendrycksTest-professional_law": 0,
    "hendrycksTest-professional_medicine": 0,
    "hendrycksTest-professional_psychology": 0,
    "hendrycksTest-public_relations": 0,
    "hendrycksTest-security_studies": 0,
    "hendrycksTest-sociology": 0,
    "hendrycksTest-us_foreign_policy": 0,
    "hendrycksTest-virology": 0,
    "hendrycksTest-world_religions": 0
  },
  "config": {
    "model": "gmukobi/shf-7b-default",
    "model_args": null,
    "num_fewshot": 0,
    "batch_size": 8,
    "device": null,
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": null
  }
}
