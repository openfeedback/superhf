{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.26,
      "acc_stderr": 0.04408440022768078,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.04688261722621504
    },
    "hendrycksTest-anatomy": {
      "acc": 0.4074074074074074,
      "acc_stderr": 0.04244633238353228,
      "acc_norm": 0.3037037037037037,
      "acc_norm_stderr": 0.039725528847851375
    },
    "hendrycksTest-astronomy": {
      "acc": 0.34210526315789475,
      "acc_stderr": 0.03860731599316091,
      "acc_norm": 0.4144736842105263,
      "acc_norm_stderr": 0.04008973785779206
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.51,
      "acc_stderr": 0.05024183937956911,
      "acc_norm": 0.41,
      "acc_norm_stderr": 0.049431107042371025
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.3584905660377358,
      "acc_stderr": 0.029514703583981765,
      "acc_norm": 0.3849056603773585,
      "acc_norm_stderr": 0.02994649856769995
    },
    "hendrycksTest-college_biology": {
      "acc": 0.3541666666666667,
      "acc_stderr": 0.039994111357535424,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.039420826399272135
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.23,
      "acc_stderr": 0.04229525846816506,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.044084400227680794
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.27,
      "acc_stderr": 0.04461960433384741,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.045604802157206845
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.28,
      "acc_stderr": 0.045126085985421276,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.04408440022768078
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.34104046242774566,
      "acc_stderr": 0.036146654241808254,
      "acc_norm": 0.3236994219653179,
      "acc_norm_stderr": 0.0356760379963917
    },
    "hendrycksTest-college_physics": {
      "acc": 0.21568627450980393,
      "acc_stderr": 0.04092563958237655,
      "acc_norm": 0.29411764705882354,
      "acc_norm_stderr": 0.04533838195929775
    },
    "hendrycksTest-computer_security": {
      "acc": 0.43,
      "acc_stderr": 0.04975698519562428,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.049604496374885836
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.3446808510638298,
      "acc_stderr": 0.03106898596312215,
      "acc_norm": 0.23829787234042554,
      "acc_norm_stderr": 0.027851252973889767
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2982456140350877,
      "acc_stderr": 0.04303684033537314,
      "acc_norm": 0.18421052631578946,
      "acc_norm_stderr": 0.03646758875075566
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.3931034482758621,
      "acc_stderr": 0.0407032901370707,
      "acc_norm": 0.32413793103448274,
      "acc_norm_stderr": 0.03900432069185554
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.30158730158730157,
      "acc_stderr": 0.0236369759961018,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.023068188848261107
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.38095238095238093,
      "acc_stderr": 0.043435254289490986,
      "acc_norm": 0.3412698412698413,
      "acc_norm_stderr": 0.04240799327574924
    },
    "hendrycksTest-global_facts": {
      "acc": 0.3,
      "acc_stderr": 0.046056618647183814,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.3709677419354839,
      "acc_stderr": 0.027480541887953593,
      "acc_norm": 0.3419354838709677,
      "acc_norm_stderr": 0.026985289576552732
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.2660098522167488,
      "acc_stderr": 0.031089826002937523,
      "acc_norm": 0.30049261083743845,
      "acc_norm_stderr": 0.03225799476233484
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.38,
      "acc_stderr": 0.048783173121456316,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.049236596391733084
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.4,
      "acc_stderr": 0.03825460278380026,
      "acc_norm": 0.38181818181818183,
      "acc_norm_stderr": 0.037937131711656344
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.35858585858585856,
      "acc_stderr": 0.03416903640391521,
      "acc_norm": 0.36363636363636365,
      "acc_norm_stderr": 0.03427308652999934
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.40932642487046633,
      "acc_stderr": 0.03548608168860806,
      "acc_norm": 0.35233160621761656,
      "acc_norm_stderr": 0.034474782864143586
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.3435897435897436,
      "acc_stderr": 0.024078696580635467,
      "acc_norm": 0.29743589743589743,
      "acc_norm_stderr": 0.02317740813146594
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.23333333333333334,
      "acc_stderr": 0.02578787422095932,
      "acc_norm": 0.29259259259259257,
      "acc_norm_stderr": 0.02773896963217609
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.3445378151260504,
      "acc_stderr": 0.030868682604121626,
      "acc_norm": 0.36134453781512604,
      "acc_norm_stderr": 0.031204691225150016
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.19205298013245034,
      "acc_stderr": 0.03216298420593615,
      "acc_norm": 0.2119205298013245,
      "acc_norm_stderr": 0.03336767086567978
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.43853211009174314,
      "acc_stderr": 0.021274713073954565,
      "acc_norm": 0.3321100917431193,
      "acc_norm_stderr": 0.020192682985423347
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.3055555555555556,
      "acc_stderr": 0.031415546294025445,
      "acc_norm": 0.2916666666666667,
      "acc_norm_stderr": 0.03099866630456052
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.4264705882352941,
      "acc_stderr": 0.03471157907953424,
      "acc_norm": 0.39705882352941174,
      "acc_norm_stderr": 0.034341311647191286
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.4472573839662447,
      "acc_stderr": 0.03236564251614192,
      "acc_norm": 0.41350210970464135,
      "acc_norm_stderr": 0.03205649904851859
    },
    "hendrycksTest-human_aging": {
      "acc": 0.4170403587443946,
      "acc_stderr": 0.03309266936071721,
      "acc_norm": 0.29596412556053814,
      "acc_norm_stderr": 0.030636591348699813
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.46564885496183206,
      "acc_stderr": 0.043749285605997376,
      "acc_norm": 0.3435114503816794,
      "acc_norm_stderr": 0.041649760719448786
    },
    "hendrycksTest-international_law": {
      "acc": 0.3884297520661157,
      "acc_stderr": 0.044492703500683815,
      "acc_norm": 0.5785123966942148,
      "acc_norm_stderr": 0.045077322787750874
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.42592592592592593,
      "acc_stderr": 0.047803436269367894,
      "acc_norm": 0.5092592592592593,
      "acc_norm_stderr": 0.04832853553437055
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.34355828220858897,
      "acc_stderr": 0.03731133519673891,
      "acc_norm": 0.3803680981595092,
      "acc_norm_stderr": 0.03814269893261837
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.36607142857142855,
      "acc_stderr": 0.045723723587374296,
      "acc_norm": 0.3392857142857143,
      "acc_norm_stderr": 0.04493949068613539
    },
    "hendrycksTest-management": {
      "acc": 0.44660194174757284,
      "acc_stderr": 0.04922424153458934,
      "acc_norm": 0.36893203883495146,
      "acc_norm_stderr": 0.0477761518115674
    },
    "hendrycksTest-marketing": {
      "acc": 0.6068376068376068,
      "acc_stderr": 0.03199957924651048,
      "acc_norm": 0.5341880341880342,
      "acc_norm_stderr": 0.03267942734081228
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.35,
      "acc_stderr": 0.04793724854411019,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.048523658709391
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.5376756066411239,
      "acc_stderr": 0.017829131764287184,
      "acc_norm": 0.4227330779054917,
      "acc_norm_stderr": 0.017665180351954062
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.3236994219653179,
      "acc_stderr": 0.025190181327608408,
      "acc_norm": 0.3179190751445087,
      "acc_norm_stderr": 0.025070713719153183
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.23798882681564246,
      "acc_stderr": 0.014242630070574915,
      "acc_norm": 0.31731843575418994,
      "acc_norm_stderr": 0.01556639263005703
    },
    "hendrycksTest-nutrition": {
      "acc": 0.38562091503267976,
      "acc_stderr": 0.027870745278290313,
      "acc_norm": 0.43790849673202614,
      "acc_norm_stderr": 0.02840830202033269
    },
    "hendrycksTest-philosophy": {
      "acc": 0.3665594855305466,
      "acc_stderr": 0.027368078243971635,
      "acc_norm": 0.3311897106109325,
      "acc_norm_stderr": 0.026730620728004924
    },
    "hendrycksTest-prehistory": {
      "acc": 0.42901234567901236,
      "acc_stderr": 0.027538925613470867,
      "acc_norm": 0.3395061728395062,
      "acc_norm_stderr": 0.026348564412011624
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.28368794326241137,
      "acc_stderr": 0.026891709428343957,
      "acc_norm": 0.2553191489361702,
      "acc_norm_stderr": 0.02601199293090201
    },
    "hendrycksTest-professional_law": {
      "acc": 0.29791395045632335,
      "acc_stderr": 0.01168071734040004,
      "acc_norm": 0.3050847457627119,
      "acc_norm_stderr": 0.011759939618085458
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.2757352941176471,
      "acc_stderr": 0.027146271936625162,
      "acc_norm": 0.2610294117647059,
      "acc_norm_stderr": 0.02667925227010313
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.3366013071895425,
      "acc_stderr": 0.019117213911495165,
      "acc_norm": 0.3088235294117647,
      "acc_norm_stderr": 0.01869085027359528
    },
    "hendrycksTest-public_relations": {
      "acc": 0.4727272727272727,
      "acc_stderr": 0.04782001791380063,
      "acc_norm": 0.2636363636363636,
      "acc_norm_stderr": 0.04220224692971987
    },
    "hendrycksTest-security_studies": {
      "acc": 0.4204081632653061,
      "acc_stderr": 0.03160106993449603,
      "acc_norm": 0.3142857142857143,
      "acc_norm_stderr": 0.029719329422417475
    },
    "hendrycksTest-sociology": {
      "acc": 0.48258706467661694,
      "acc_stderr": 0.03533389234739245,
      "acc_norm": 0.4079601990049751,
      "acc_norm_stderr": 0.034751163651940926
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.52,
      "acc_stderr": 0.05021167315686781,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.050251890762960605
    },
    "hendrycksTest-virology": {
      "acc": 0.3795180722891566,
      "acc_stderr": 0.03777798822748017,
      "acc_norm": 0.3433734939759036,
      "acc_norm_stderr": 0.03696584317010601
    },
    "hendrycksTest-world_religions": {
      "acc": 0.631578947368421,
      "acc_stderr": 0.036996580176568775,
      "acc_norm": 0.5672514619883041,
      "acc_norm_stderr": 0.03799978644370607
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 0,
    "hendrycksTest-anatomy": 0,
    "hendrycksTest-astronomy": 0,
    "hendrycksTest-business_ethics": 0,
    "hendrycksTest-clinical_knowledge": 0,
    "hendrycksTest-college_biology": 0,
    "hendrycksTest-college_chemistry": 0,
    "hendrycksTest-college_computer_science": 0,
    "hendrycksTest-college_mathematics": 0,
    "hendrycksTest-college_medicine": 0,
    "hendrycksTest-college_physics": 0,
    "hendrycksTest-computer_security": 0,
    "hendrycksTest-conceptual_physics": 0,
    "hendrycksTest-econometrics": 0,
    "hendrycksTest-electrical_engineering": 0,
    "hendrycksTest-elementary_mathematics": 0,
    "hendrycksTest-formal_logic": 0,
    "hendrycksTest-global_facts": 0,
    "hendrycksTest-high_school_biology": 0,
    "hendrycksTest-high_school_chemistry": 0,
    "hendrycksTest-high_school_computer_science": 0,
    "hendrycksTest-high_school_european_history": 0,
    "hendrycksTest-high_school_geography": 0,
    "hendrycksTest-high_school_government_and_politics": 0,
    "hendrycksTest-high_school_macroeconomics": 0,
    "hendrycksTest-high_school_mathematics": 0,
    "hendrycksTest-high_school_microeconomics": 0,
    "hendrycksTest-high_school_physics": 0,
    "hendrycksTest-high_school_psychology": 0,
    "hendrycksTest-high_school_statistics": 0,
    "hendrycksTest-high_school_us_history": 0,
    "hendrycksTest-high_school_world_history": 0,
    "hendrycksTest-human_aging": 0,
    "hendrycksTest-human_sexuality": 0,
    "hendrycksTest-international_law": 0,
    "hendrycksTest-jurisprudence": 0,
    "hendrycksTest-logical_fallacies": 0,
    "hendrycksTest-machine_learning": 0,
    "hendrycksTest-management": 0,
    "hendrycksTest-marketing": 0,
    "hendrycksTest-medical_genetics": 0,
    "hendrycksTest-miscellaneous": 0,
    "hendrycksTest-moral_disputes": 0,
    "hendrycksTest-moral_scenarios": 0,
    "hendrycksTest-nutrition": 0,
    "hendrycksTest-philosophy": 0,
    "hendrycksTest-prehistory": 0,
    "hendrycksTest-professional_accounting": 0,
    "hendrycksTest-professional_law": 0,
    "hendrycksTest-professional_medicine": 0,
    "hendrycksTest-professional_psychology": 0,
    "hendrycksTest-public_relations": 0,
    "hendrycksTest-security_studies": 0,
    "hendrycksTest-sociology": 0,
    "hendrycksTest-us_foreign_policy": 0,
    "hendrycksTest-virology": 0,
    "hendrycksTest-world_religions": 0
  },
  "config": {
    "model": "peterchatain/rlhf-v3-lr-5.0e-6-batch-16@gold-run",
    "model_args": null,
    "num_fewshot": 0,
    "batch_size": 8,
    "device": null,
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": null
  }
}
