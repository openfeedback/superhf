{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.3,
      "acc_stderr": 0.046056618647183814,
      "acc_norm": 0.23,
      "acc_norm_stderr": 0.04229525846816505
    },
    "hendrycksTest-anatomy": {
      "acc": 0.2962962962962963,
      "acc_stderr": 0.03944624162501117,
      "acc_norm": 0.2518518518518518,
      "acc_norm_stderr": 0.03749850709174023
    },
    "hendrycksTest-astronomy": {
      "acc": 0.39473684210526316,
      "acc_stderr": 0.039777499346220734,
      "acc_norm": 0.40789473684210525,
      "acc_norm_stderr": 0.03999309712777471
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.43,
      "acc_stderr": 0.049756985195624284,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.04852365870939099
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.35471698113207545,
      "acc_stderr": 0.029445175328199586,
      "acc_norm": 0.3283018867924528,
      "acc_norm_stderr": 0.028901593612411784
    },
    "hendrycksTest-college_biology": {
      "acc": 0.2638888888888889,
      "acc_stderr": 0.03685651095897532,
      "acc_norm": 0.2708333333333333,
      "acc_norm_stderr": 0.037161774375660185
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.24,
      "acc_stderr": 0.042923469599092816,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.38,
      "acc_stderr": 0.048783173121456316,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.046882617226215034
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.21,
      "acc_stderr": 0.040936018074033256,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.044084400227680794
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.3179190751445087,
      "acc_stderr": 0.0355068398916558,
      "acc_norm": 0.2658959537572254,
      "acc_norm_stderr": 0.03368762932259431
    },
    "hendrycksTest-college_physics": {
      "acc": 0.2647058823529412,
      "acc_stderr": 0.04389869956808778,
      "acc_norm": 0.3431372549019608,
      "acc_norm_stderr": 0.04724007352383888
    },
    "hendrycksTest-computer_security": {
      "acc": 0.34,
      "acc_stderr": 0.04760952285695236,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.0479372485441102
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.28936170212765955,
      "acc_stderr": 0.02964400657700962,
      "acc_norm": 0.2127659574468085,
      "acc_norm_stderr": 0.026754391348039766
    },
    "hendrycksTest-econometrics": {
      "acc": 0.32456140350877194,
      "acc_stderr": 0.04404556157374767,
      "acc_norm": 0.19298245614035087,
      "acc_norm_stderr": 0.037124548537213684
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.33793103448275863,
      "acc_stderr": 0.039417076320648906,
      "acc_norm": 0.3310344827586207,
      "acc_norm_stderr": 0.039215453124671215
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.29365079365079366,
      "acc_stderr": 0.023456037383982036,
      "acc_norm": 0.2724867724867725,
      "acc_norm_stderr": 0.022930973071633356
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.30158730158730157,
      "acc_stderr": 0.04104947269903394,
      "acc_norm": 0.30158730158730157,
      "acc_norm_stderr": 0.04104947269903394
    },
    "hendrycksTest-global_facts": {
      "acc": 0.21,
      "acc_stderr": 0.04093601807403326,
      "acc_norm": 0.21,
      "acc_norm_stderr": 0.04093601807403326
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.3,
      "acc_stderr": 0.026069362295335137,
      "acc_norm": 0.3096774193548387,
      "acc_norm_stderr": 0.026302774983517418
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.24630541871921183,
      "acc_stderr": 0.030315099285617732,
      "acc_norm": 0.2857142857142857,
      "acc_norm_stderr": 0.0317852971064275
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.26,
      "acc_stderr": 0.04408440022768079,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.04725815626252605
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.3212121212121212,
      "acc_stderr": 0.03646204963253813,
      "acc_norm": 0.32727272727272727,
      "acc_norm_stderr": 0.03663974994391242
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.3181818181818182,
      "acc_stderr": 0.03318477333845331,
      "acc_norm": 0.31313131313131315,
      "acc_norm_stderr": 0.033042050878136525
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.27979274611398963,
      "acc_stderr": 0.032396370467357015,
      "acc_norm": 0.29533678756476683,
      "acc_norm_stderr": 0.03292296639155141
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.31794871794871793,
      "acc_stderr": 0.02361088430892786,
      "acc_norm": 0.30256410256410254,
      "acc_norm_stderr": 0.023290888053772735
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.21481481481481482,
      "acc_stderr": 0.02504044387700069,
      "acc_norm": 0.3111111111111111,
      "acc_norm_stderr": 0.028226446749683515
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.29831932773109243,
      "acc_stderr": 0.02971914287634285,
      "acc_norm": 0.3697478991596639,
      "acc_norm_stderr": 0.031357095996135904
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.23841059602649006,
      "acc_stderr": 0.03479185572599661,
      "acc_norm": 0.271523178807947,
      "acc_norm_stderr": 0.03631329803969653
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.3467889908256881,
      "acc_stderr": 0.020406097104093027,
      "acc_norm": 0.27155963302752295,
      "acc_norm_stderr": 0.019069098363191445
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.3148148148148148,
      "acc_stderr": 0.03167468706828979,
      "acc_norm": 0.3101851851851852,
      "acc_norm_stderr": 0.03154696285656628
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.3382352941176471,
      "acc_stderr": 0.033205746129454324,
      "acc_norm": 0.31862745098039214,
      "acc_norm_stderr": 0.0327028718148208
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.2911392405063291,
      "acc_stderr": 0.029571601065753374,
      "acc_norm": 0.3080168776371308,
      "acc_norm_stderr": 0.0300523893356057
    },
    "hendrycksTest-human_aging": {
      "acc": 0.26905829596412556,
      "acc_stderr": 0.029763779406874972,
      "acc_norm": 0.21524663677130046,
      "acc_norm_stderr": 0.02758406660220826
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.42748091603053434,
      "acc_stderr": 0.04338920305792401,
      "acc_norm": 0.3282442748091603,
      "acc_norm_stderr": 0.04118438565806298
    },
    "hendrycksTest-international_law": {
      "acc": 0.3305785123966942,
      "acc_stderr": 0.04294340845212095,
      "acc_norm": 0.5371900826446281,
      "acc_norm_stderr": 0.04551711196104218
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.37037037037037035,
      "acc_stderr": 0.04668408033024932,
      "acc_norm": 0.46296296296296297,
      "acc_norm_stderr": 0.04820403072760627
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.25766871165644173,
      "acc_stderr": 0.03436150827846917,
      "acc_norm": 0.32515337423312884,
      "acc_norm_stderr": 0.036803503712864616
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.25892857142857145,
      "acc_stderr": 0.041577515398656284,
      "acc_norm": 0.26785714285714285,
      "acc_norm_stderr": 0.04203277291467762
    },
    "hendrycksTest-management": {
      "acc": 0.2815533980582524,
      "acc_stderr": 0.04453254836326469,
      "acc_norm": 0.3106796116504854,
      "acc_norm_stderr": 0.04582124160161552
    },
    "hendrycksTest-marketing": {
      "acc": 0.5170940170940171,
      "acc_stderr": 0.032736940493481824,
      "acc_norm": 0.46153846153846156,
      "acc_norm_stderr": 0.032659033811861936
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.32,
      "acc_stderr": 0.046882617226215034,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.04824181513244218
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.4099616858237548,
      "acc_stderr": 0.01758767231233605,
      "acc_norm": 0.3243933588761175,
      "acc_norm_stderr": 0.016740929047162706
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.26011560693641617,
      "acc_stderr": 0.023618678310069374,
      "acc_norm": 0.3208092485549133,
      "acc_norm_stderr": 0.02513100023364792
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.23798882681564246,
      "acc_stderr": 0.014242630070574915,
      "acc_norm": 0.2737430167597765,
      "acc_norm_stderr": 0.014912413096372428
    },
    "hendrycksTest-nutrition": {
      "acc": 0.4019607843137255,
      "acc_stderr": 0.028074158947600656,
      "acc_norm": 0.42483660130718953,
      "acc_norm_stderr": 0.02830457667314111
    },
    "hendrycksTest-philosophy": {
      "acc": 0.28938906752411575,
      "acc_stderr": 0.025755865922632928,
      "acc_norm": 0.3183279742765273,
      "acc_norm_stderr": 0.02645722506781103
    },
    "hendrycksTest-prehistory": {
      "acc": 0.3117283950617284,
      "acc_stderr": 0.025773111169630436,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.024922001168886345
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.25886524822695034,
      "acc_stderr": 0.026129572527180848,
      "acc_norm": 0.3049645390070922,
      "acc_norm_stderr": 0.027464708442022142
    },
    "hendrycksTest-professional_law": {
      "acc": 0.25684485006518903,
      "acc_stderr": 0.01115845585309885,
      "acc_norm": 0.28683181225554105,
      "acc_norm_stderr": 0.011551504781176933
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.28308823529411764,
      "acc_stderr": 0.02736586113151381,
      "acc_norm": 0.29044117647058826,
      "acc_norm_stderr": 0.02757646862274052
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.3104575163398693,
      "acc_stderr": 0.018718067052623234,
      "acc_norm": 0.2696078431372549,
      "acc_norm_stderr": 0.017952449196987862
    },
    "hendrycksTest-public_relations": {
      "acc": 0.3181818181818182,
      "acc_stderr": 0.04461272175910508,
      "acc_norm": 0.19090909090909092,
      "acc_norm_stderr": 0.03764425585984925
    },
    "hendrycksTest-security_studies": {
      "acc": 0.42448979591836733,
      "acc_stderr": 0.03164209487942942,
      "acc_norm": 0.3306122448979592,
      "acc_norm_stderr": 0.030116426296540596
    },
    "hendrycksTest-sociology": {
      "acc": 0.29850746268656714,
      "acc_stderr": 0.032357437893550424,
      "acc_norm": 0.29850746268656714,
      "acc_norm_stderr": 0.03235743789355041
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.45,
      "acc_stderr": 0.049999999999999996,
      "acc_norm": 0.45,
      "acc_norm_stderr": 0.05
    },
    "hendrycksTest-virology": {
      "acc": 0.3674698795180723,
      "acc_stderr": 0.03753267402120574,
      "acc_norm": 0.29518072289156627,
      "acc_norm_stderr": 0.03550920185689631
    },
    "hendrycksTest-world_religions": {
      "acc": 0.49122807017543857,
      "acc_stderr": 0.03834234744164993,
      "acc_norm": 0.49122807017543857,
      "acc_norm_stderr": 0.03834234744164993
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 0,
    "hendrycksTest-anatomy": 0,
    "hendrycksTest-astronomy": 0,
    "hendrycksTest-business_ethics": 0,
    "hendrycksTest-clinical_knowledge": 0,
    "hendrycksTest-college_biology": 0,
    "hendrycksTest-college_chemistry": 0,
    "hendrycksTest-college_computer_science": 0,
    "hendrycksTest-college_mathematics": 0,
    "hendrycksTest-college_medicine": 0,
    "hendrycksTest-college_physics": 0,
    "hendrycksTest-computer_security": 0,
    "hendrycksTest-conceptual_physics": 0,
    "hendrycksTest-econometrics": 0,
    "hendrycksTest-electrical_engineering": 0,
    "hendrycksTest-elementary_mathematics": 0,
    "hendrycksTest-formal_logic": 0,
    "hendrycksTest-global_facts": 0,
    "hendrycksTest-high_school_biology": 0,
    "hendrycksTest-high_school_chemistry": 0,
    "hendrycksTest-high_school_computer_science": 0,
    "hendrycksTest-high_school_european_history": 0,
    "hendrycksTest-high_school_geography": 0,
    "hendrycksTest-high_school_government_and_politics": 0,
    "hendrycksTest-high_school_macroeconomics": 0,
    "hendrycksTest-high_school_mathematics": 0,
    "hendrycksTest-high_school_microeconomics": 0,
    "hendrycksTest-high_school_physics": 0,
    "hendrycksTest-high_school_psychology": 0,
    "hendrycksTest-high_school_statistics": 0,
    "hendrycksTest-high_school_us_history": 0,
    "hendrycksTest-high_school_world_history": 0,
    "hendrycksTest-human_aging": 0,
    "hendrycksTest-human_sexuality": 0,
    "hendrycksTest-international_law": 0,
    "hendrycksTest-jurisprudence": 0,
    "hendrycksTest-logical_fallacies": 0,
    "hendrycksTest-machine_learning": 0,
    "hendrycksTest-management": 0,
    "hendrycksTest-marketing": 0,
    "hendrycksTest-medical_genetics": 0,
    "hendrycksTest-miscellaneous": 0,
    "hendrycksTest-moral_disputes": 0,
    "hendrycksTest-moral_scenarios": 0,
    "hendrycksTest-nutrition": 0,
    "hendrycksTest-philosophy": 0,
    "hendrycksTest-prehistory": 0,
    "hendrycksTest-professional_accounting": 0,
    "hendrycksTest-professional_law": 0,
    "hendrycksTest-professional_medicine": 0,
    "hendrycksTest-professional_psychology": 0,
    "hendrycksTest-public_relations": 0,
    "hendrycksTest-security_studies": 0,
    "hendrycksTest-sociology": 0,
    "hendrycksTest-us_foreign_policy": 0,
    "hendrycksTest-virology": 0,
    "hendrycksTest-world_religions": 0
  },
  "config": {
    "model": "peterchatain/rlhf-fixed-llama-instruct-bs-16",
    "model_args": null,
    "num_fewshot": 0,
    "batch_size": 8,
    "device": null,
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": null
  }
}
