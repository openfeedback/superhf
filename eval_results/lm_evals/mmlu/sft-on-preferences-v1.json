{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.26,
      "acc_stderr": 0.04408440022768078,
      "acc_norm": 0.23,
      "acc_norm_stderr": 0.04229525846816505
    },
    "hendrycksTest-anatomy": {
      "acc": 0.37037037037037035,
      "acc_stderr": 0.041716541613545426,
      "acc_norm": 0.31851851851851853,
      "acc_norm_stderr": 0.04024778401977111
    },
    "hendrycksTest-astronomy": {
      "acc": 0.3157894736842105,
      "acc_stderr": 0.0378272898086547,
      "acc_norm": 0.4276315789473684,
      "acc_norm_stderr": 0.04026097083296559
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.51,
      "acc_stderr": 0.05024183937956911,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.049604496374885836
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.3283018867924528,
      "acc_stderr": 0.02890159361241178,
      "acc_norm": 0.3471698113207547,
      "acc_norm_stderr": 0.029300101705549655
    },
    "hendrycksTest-college_biology": {
      "acc": 0.2638888888888889,
      "acc_stderr": 0.03685651095897532,
      "acc_norm": 0.2847222222222222,
      "acc_norm_stderr": 0.037738099906869334
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.23,
      "acc_stderr": 0.042295258468165065,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.26,
      "acc_stderr": 0.044084400227680794,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.045126085985421276
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.3,
      "acc_stderr": 0.046056618647183814,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.32947976878612717,
      "acc_stderr": 0.03583901754736412,
      "acc_norm": 0.28901734104046245,
      "acc_norm_stderr": 0.03456425745087
    },
    "hendrycksTest-college_physics": {
      "acc": 0.23529411764705882,
      "acc_stderr": 0.04220773659171451,
      "acc_norm": 0.29411764705882354,
      "acc_norm_stderr": 0.04533838195929776
    },
    "hendrycksTest-computer_security": {
      "acc": 0.37,
      "acc_stderr": 0.048523658709391,
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.04878317312145632
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.3021276595744681,
      "acc_stderr": 0.030017554471880554,
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.026148818018424506
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2982456140350877,
      "acc_stderr": 0.043036840335373146,
      "acc_norm": 0.18421052631578946,
      "acc_norm_stderr": 0.03646758875075566
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.3586206896551724,
      "acc_stderr": 0.039966295748767186,
      "acc_norm": 0.32413793103448274,
      "acc_norm_stderr": 0.03900432069185554
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.2830687830687831,
      "acc_stderr": 0.023201392938194974,
      "acc_norm": 0.26455026455026454,
      "acc_norm_stderr": 0.022717467897708624
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.38095238095238093,
      "acc_stderr": 0.04343525428949099,
      "acc_norm": 0.3412698412698413,
      "acc_norm_stderr": 0.04240799327574924
    },
    "hendrycksTest-global_facts": {
      "acc": 0.22,
      "acc_stderr": 0.041633319989322695,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.044084400227680794
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.3258064516129032,
      "acc_stderr": 0.026662010578567107,
      "acc_norm": 0.3225806451612903,
      "acc_norm_stderr": 0.026593084516572267
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.22167487684729065,
      "acc_stderr": 0.02922557589248962,
      "acc_norm": 0.2857142857142857,
      "acc_norm_stderr": 0.031785297106427496
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.32,
      "acc_stderr": 0.046882617226215034,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.048523658709391
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.3939393939393939,
      "acc_stderr": 0.038154943086889305,
      "acc_norm": 0.3696969696969697,
      "acc_norm_stderr": 0.03769430314512568
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.03358618145732523,
      "acc_norm": 0.3282828282828283,
      "acc_norm_stderr": 0.03345678422756777
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.35751295336787564,
      "acc_stderr": 0.03458816042181005,
      "acc_norm": 0.31088082901554404,
      "acc_norm_stderr": 0.03340361906276588
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.31025641025641026,
      "acc_stderr": 0.02345467488940429,
      "acc_norm": 0.31025641025641026,
      "acc_norm_stderr": 0.02345467488940429
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.23333333333333334,
      "acc_stderr": 0.025787874220959323,
      "acc_norm": 0.2962962962962963,
      "acc_norm_stderr": 0.027840811495871923
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.3487394957983193,
      "acc_stderr": 0.03095663632856655,
      "acc_norm": 0.35714285714285715,
      "acc_norm_stderr": 0.031124619309328177
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2251655629139073,
      "acc_stderr": 0.03410435282008937,
      "acc_norm": 0.23841059602649006,
      "acc_norm_stderr": 0.034791855725996586
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.363302752293578,
      "acc_stderr": 0.020620603919625804,
      "acc_norm": 0.3192660550458716,
      "acc_norm_stderr": 0.019987829069750003
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.3055555555555556,
      "acc_stderr": 0.031415546294025445,
      "acc_norm": 0.3287037037037037,
      "acc_norm_stderr": 0.032036140846700596
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.37745098039215685,
      "acc_stderr": 0.03402272044340705,
      "acc_norm": 0.35784313725490197,
      "acc_norm_stderr": 0.03364487286088299
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.3881856540084388,
      "acc_stderr": 0.031722950043323296,
      "acc_norm": 0.3670886075949367,
      "acc_norm_stderr": 0.031376240725616185
    },
    "hendrycksTest-human_aging": {
      "acc": 0.33183856502242154,
      "acc_stderr": 0.031602951437766785,
      "acc_norm": 0.273542600896861,
      "acc_norm_stderr": 0.029918586707798824
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.42748091603053434,
      "acc_stderr": 0.04338920305792401,
      "acc_norm": 0.3511450381679389,
      "acc_norm_stderr": 0.04186445163013751
    },
    "hendrycksTest-international_law": {
      "acc": 0.35537190082644626,
      "acc_stderr": 0.04369236326573981,
      "acc_norm": 0.5619834710743802,
      "acc_norm_stderr": 0.04529146804435792
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.4166666666666667,
      "acc_stderr": 0.047660751653564606,
      "acc_norm": 0.5092592592592593,
      "acc_norm_stderr": 0.04832853553437055
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.31901840490797545,
      "acc_stderr": 0.03661997551073836,
      "acc_norm": 0.3558282208588957,
      "acc_norm_stderr": 0.03761521380046734
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.3392857142857143,
      "acc_stderr": 0.04493949068613539,
      "acc_norm": 0.25892857142857145,
      "acc_norm_stderr": 0.041577515398656284
    },
    "hendrycksTest-management": {
      "acc": 0.3300970873786408,
      "acc_stderr": 0.04656147110012351,
      "acc_norm": 0.3592233009708738,
      "acc_norm_stderr": 0.04750458399041696
    },
    "hendrycksTest-marketing": {
      "acc": 0.5512820512820513,
      "acc_stderr": 0.032583346493868806,
      "acc_norm": 0.5128205128205128,
      "acc_norm_stderr": 0.0327453193884235
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.32,
      "acc_stderr": 0.04688261722621504,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.047937248544110196
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.4904214559386973,
      "acc_stderr": 0.01787668227534088,
      "acc_norm": 0.3716475095785441,
      "acc_norm_stderr": 0.01728080252213319
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.2745664739884393,
      "acc_stderr": 0.024027745155265002,
      "acc_norm": 0.315028901734104,
      "acc_norm_stderr": 0.0250093137900697
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.23798882681564246,
      "acc_stderr": 0.014242630070574915,
      "acc_norm": 0.27262569832402234,
      "acc_norm_stderr": 0.014893391735249588
    },
    "hendrycksTest-nutrition": {
      "acc": 0.4019607843137255,
      "acc_stderr": 0.028074158947600666,
      "acc_norm": 0.42483660130718953,
      "acc_norm_stderr": 0.028304576673141117
    },
    "hendrycksTest-philosophy": {
      "acc": 0.3054662379421222,
      "acc_stderr": 0.026160584450140488,
      "acc_norm": 0.3311897106109325,
      "acc_norm_stderr": 0.026730620728004917
    },
    "hendrycksTest-prehistory": {
      "acc": 0.35802469135802467,
      "acc_stderr": 0.026675611926037096,
      "acc_norm": 0.3055555555555556,
      "acc_norm_stderr": 0.025630824975621344
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.2872340425531915,
      "acc_stderr": 0.026992199173064356,
      "acc_norm": 0.2695035460992908,
      "acc_norm_stderr": 0.026469036818590634
    },
    "hendrycksTest-professional_law": {
      "acc": 0.2894393741851369,
      "acc_stderr": 0.011582659702210245,
      "acc_norm": 0.29335071707953064,
      "acc_norm_stderr": 0.011628520449582073
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.23161764705882354,
      "acc_stderr": 0.025626533803777562,
      "acc_norm": 0.2977941176470588,
      "acc_norm_stderr": 0.02777829870154544
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.3202614379084967,
      "acc_stderr": 0.018875682938069443,
      "acc_norm": 0.28921568627450983,
      "acc_norm_stderr": 0.01834252984527591
    },
    "hendrycksTest-public_relations": {
      "acc": 0.35454545454545455,
      "acc_stderr": 0.04582004841505417,
      "acc_norm": 0.24545454545454545,
      "acc_norm_stderr": 0.04122066502878284
    },
    "hendrycksTest-security_studies": {
      "acc": 0.40816326530612246,
      "acc_stderr": 0.03146465712827423,
      "acc_norm": 0.2979591836734694,
      "acc_norm_stderr": 0.029279567411065664
    },
    "hendrycksTest-sociology": {
      "acc": 0.3582089552238806,
      "acc_stderr": 0.03390393042268814,
      "acc_norm": 0.36318407960199006,
      "acc_norm_stderr": 0.034005985055990146
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.48,
      "acc_stderr": 0.050211673156867795,
      "acc_norm": 0.45,
      "acc_norm_stderr": 0.05
    },
    "hendrycksTest-virology": {
      "acc": 0.3313253012048193,
      "acc_stderr": 0.036643147772880864,
      "acc_norm": 0.30120481927710846,
      "acc_norm_stderr": 0.0357160923005348
    },
    "hendrycksTest-world_religions": {
      "acc": 0.5555555555555556,
      "acc_stderr": 0.03811079669833531,
      "acc_norm": 0.543859649122807,
      "acc_norm_stderr": 0.03820042586602966
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 0,
    "hendrycksTest-anatomy": 0,
    "hendrycksTest-astronomy": 0,
    "hendrycksTest-business_ethics": 0,
    "hendrycksTest-clinical_knowledge": 0,
    "hendrycksTest-college_biology": 0,
    "hendrycksTest-college_chemistry": 0,
    "hendrycksTest-college_computer_science": 0,
    "hendrycksTest-college_mathematics": 0,
    "hendrycksTest-college_medicine": 0,
    "hendrycksTest-college_physics": 0,
    "hendrycksTest-computer_security": 0,
    "hendrycksTest-conceptual_physics": 0,
    "hendrycksTest-econometrics": 0,
    "hendrycksTest-electrical_engineering": 0,
    "hendrycksTest-elementary_mathematics": 0,
    "hendrycksTest-formal_logic": 0,
    "hendrycksTest-global_facts": 0,
    "hendrycksTest-high_school_biology": 0,
    "hendrycksTest-high_school_chemistry": 0,
    "hendrycksTest-high_school_computer_science": 0,
    "hendrycksTest-high_school_european_history": 0,
    "hendrycksTest-high_school_geography": 0,
    "hendrycksTest-high_school_government_and_politics": 0,
    "hendrycksTest-high_school_macroeconomics": 0,
    "hendrycksTest-high_school_mathematics": 0,
    "hendrycksTest-high_school_microeconomics": 0,
    "hendrycksTest-high_school_physics": 0,
    "hendrycksTest-high_school_psychology": 0,
    "hendrycksTest-high_school_statistics": 0,
    "hendrycksTest-high_school_us_history": 0,
    "hendrycksTest-high_school_world_history": 0,
    "hendrycksTest-human_aging": 0,
    "hendrycksTest-human_sexuality": 0,
    "hendrycksTest-international_law": 0,
    "hendrycksTest-jurisprudence": 0,
    "hendrycksTest-logical_fallacies": 0,
    "hendrycksTest-machine_learning": 0,
    "hendrycksTest-management": 0,
    "hendrycksTest-marketing": 0,
    "hendrycksTest-medical_genetics": 0,
    "hendrycksTest-miscellaneous": 0,
    "hendrycksTest-moral_disputes": 0,
    "hendrycksTest-moral_scenarios": 0,
    "hendrycksTest-nutrition": 0,
    "hendrycksTest-philosophy": 0,
    "hendrycksTest-prehistory": 0,
    "hendrycksTest-professional_accounting": 0,
    "hendrycksTest-professional_law": 0,
    "hendrycksTest-professional_medicine": 0,
    "hendrycksTest-professional_psychology": 0,
    "hendrycksTest-public_relations": 0,
    "hendrycksTest-security_studies": 0,
    "hendrycksTest-sociology": 0,
    "hendrycksTest-us_foreign_policy": 0,
    "hendrycksTest-virology": 0,
    "hendrycksTest-world_religions": 0
  },
  "config": {
    "model": "gmukobi/sft-on-preferences-v1",
    "model_args": null,
    "num_fewshot": 0,
    "batch_size": 8,
    "device": null,
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": null
  }
}
