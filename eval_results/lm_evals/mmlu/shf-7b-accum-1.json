{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.28,
      "acc_stderr": 0.045126085985421276,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.04688261722621504
    },
    "hendrycksTest-anatomy": {
      "acc": 0.3925925925925926,
      "acc_stderr": 0.04218506215368879,
      "acc_norm": 0.28888888888888886,
      "acc_norm_stderr": 0.0391545063041425
    },
    "hendrycksTest-astronomy": {
      "acc": 0.34210526315789475,
      "acc_stderr": 0.03860731599316091,
      "acc_norm": 0.4342105263157895,
      "acc_norm_stderr": 0.0403356566784832
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.54,
      "acc_stderr": 0.05009082659620333,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.049604496374885836
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.3660377358490566,
      "acc_stderr": 0.029647813539365245,
      "acc_norm": 0.36981132075471695,
      "acc_norm_stderr": 0.02971142188010793
    },
    "hendrycksTest-college_biology": {
      "acc": 0.2847222222222222,
      "acc_stderr": 0.03773809990686934,
      "acc_norm": 0.2916666666666667,
      "acc_norm_stderr": 0.038009680605548594
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.24,
      "acc_stderr": 0.04292346959909282,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.04560480215720684
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.27,
      "acc_stderr": 0.04461960433384741,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.045604802157206845
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.26,
      "acc_stderr": 0.0440844002276808,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.04408440022768078
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.35260115606936415,
      "acc_stderr": 0.03643037168958548,
      "acc_norm": 0.30057803468208094,
      "acc_norm_stderr": 0.0349610148119118
    },
    "hendrycksTest-college_physics": {
      "acc": 0.2647058823529412,
      "acc_stderr": 0.04389869956808778,
      "acc_norm": 0.30392156862745096,
      "acc_norm_stderr": 0.04576665403207762
    },
    "hendrycksTest-computer_security": {
      "acc": 0.41,
      "acc_stderr": 0.04943110704237101,
      "acc_norm": 0.43,
      "acc_norm_stderr": 0.049756985195624284
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.3276595744680851,
      "acc_stderr": 0.030683020843231004,
      "acc_norm": 0.225531914893617,
      "acc_norm_stderr": 0.027321078417387533
    },
    "hendrycksTest-econometrics": {
      "acc": 0.30701754385964913,
      "acc_stderr": 0.043391383225798615,
      "acc_norm": 0.17543859649122806,
      "acc_norm_stderr": 0.0357795481394837
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.3724137931034483,
      "acc_stderr": 0.0402873153294756,
      "acc_norm": 0.31724137931034485,
      "acc_norm_stderr": 0.038783523721386215
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.291005291005291,
      "acc_stderr": 0.023393826500484865,
      "acc_norm": 0.2751322751322751,
      "acc_norm_stderr": 0.02300008685906864
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.3968253968253968,
      "acc_stderr": 0.0437588849272706,
      "acc_norm": 0.35714285714285715,
      "acc_norm_stderr": 0.04285714285714281
    },
    "hendrycksTest-global_facts": {
      "acc": 0.26,
      "acc_stderr": 0.04408440022768078,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542127
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.36451612903225805,
      "acc_stderr": 0.027379871229943245,
      "acc_norm": 0.33548387096774196,
      "acc_norm_stderr": 0.02686020644472435
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.22660098522167488,
      "acc_stderr": 0.029454863835292996,
      "acc_norm": 0.3054187192118227,
      "acc_norm_stderr": 0.03240661565868408
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.37,
      "acc_stderr": 0.048523658709391,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.047937248544110196
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.3878787878787879,
      "acc_stderr": 0.038049136539710114,
      "acc_norm": 0.3878787878787879,
      "acc_norm_stderr": 0.03804913653971011
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.3434343434343434,
      "acc_stderr": 0.033832012232444426,
      "acc_norm": 0.37373737373737376,
      "acc_norm_stderr": 0.03446897738659333
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.41450777202072536,
      "acc_stderr": 0.03555300319557672,
      "acc_norm": 0.3471502590673575,
      "acc_norm_stderr": 0.03435696168361355
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.33589743589743587,
      "acc_stderr": 0.023946724741563976,
      "acc_norm": 0.28974358974358977,
      "acc_norm_stderr": 0.023000628243687957
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.22962962962962963,
      "acc_stderr": 0.025644108639267617,
      "acc_norm": 0.29259259259259257,
      "acc_norm_stderr": 0.02773896963217609
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.35714285714285715,
      "acc_stderr": 0.031124619309328177,
      "acc_norm": 0.35294117647058826,
      "acc_norm_stderr": 0.031041941304059278
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.19205298013245034,
      "acc_stderr": 0.03216298420593614,
      "acc_norm": 0.2119205298013245,
      "acc_norm_stderr": 0.03336767086567978
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.42935779816513764,
      "acc_stderr": 0.021222286397236508,
      "acc_norm": 0.3339449541284404,
      "acc_norm_stderr": 0.020220554196736407
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.3101851851851852,
      "acc_stderr": 0.031546962856566295,
      "acc_norm": 0.3101851851851852,
      "acc_norm_stderr": 0.031546962856566295
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.4362745098039216,
      "acc_stderr": 0.03480693138457038,
      "acc_norm": 0.38235294117647056,
      "acc_norm_stderr": 0.03410785338904719
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.4430379746835443,
      "acc_stderr": 0.03233532777533484,
      "acc_norm": 0.4092827004219409,
      "acc_norm_stderr": 0.032007041833595914
    },
    "hendrycksTest-human_aging": {
      "acc": 0.4170403587443946,
      "acc_stderr": 0.03309266936071721,
      "acc_norm": 0.28699551569506726,
      "acc_norm_stderr": 0.030360379710291947
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.46564885496183206,
      "acc_stderr": 0.043749285605997376,
      "acc_norm": 0.35877862595419846,
      "acc_norm_stderr": 0.04206739313864908
    },
    "hendrycksTest-international_law": {
      "acc": 0.4132231404958678,
      "acc_stderr": 0.04495087843548408,
      "acc_norm": 0.5785123966942148,
      "acc_norm_stderr": 0.04507732278775087
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.4444444444444444,
      "acc_stderr": 0.04803752235190193,
      "acc_norm": 0.5092592592592593,
      "acc_norm_stderr": 0.04832853553437055
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.3374233128834356,
      "acc_stderr": 0.03714908409935575,
      "acc_norm": 0.36809815950920244,
      "acc_norm_stderr": 0.03789213935838396
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.36607142857142855,
      "acc_stderr": 0.0457237235873743,
      "acc_norm": 0.39285714285714285,
      "acc_norm_stderr": 0.046355501356099754
    },
    "hendrycksTest-management": {
      "acc": 0.4174757281553398,
      "acc_stderr": 0.04882840548212238,
      "acc_norm": 0.3592233009708738,
      "acc_norm_stderr": 0.047504583990416946
    },
    "hendrycksTest-marketing": {
      "acc": 0.5897435897435898,
      "acc_stderr": 0.03222414045241107,
      "acc_norm": 0.5042735042735043,
      "acc_norm_stderr": 0.03275489264382133
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.36,
      "acc_stderr": 0.048241815132442176,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.04760952285695236
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.5300127713920817,
      "acc_stderr": 0.01784772308664907,
      "acc_norm": 0.39208173690932313,
      "acc_norm_stderr": 0.017458524050147636
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.34104046242774566,
      "acc_stderr": 0.02552247463212161,
      "acc_norm": 0.31213872832369943,
      "acc_norm_stderr": 0.02494679222527231
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.23798882681564246,
      "acc_stderr": 0.014242630070574915,
      "acc_norm": 0.31843575418994413,
      "acc_norm_stderr": 0.015581008080360274
    },
    "hendrycksTest-nutrition": {
      "acc": 0.37254901960784315,
      "acc_stderr": 0.027684181883302895,
      "acc_norm": 0.4150326797385621,
      "acc_norm_stderr": 0.0282135041778241
    },
    "hendrycksTest-philosophy": {
      "acc": 0.34726688102893893,
      "acc_stderr": 0.027040745502307336,
      "acc_norm": 0.3183279742765273,
      "acc_norm_stderr": 0.026457225067811025
    },
    "hendrycksTest-prehistory": {
      "acc": 0.4166666666666667,
      "acc_stderr": 0.02743162372241501,
      "acc_norm": 0.3271604938271605,
      "acc_norm_stderr": 0.026105673861409818
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.29432624113475175,
      "acc_stderr": 0.02718712701150379,
      "acc_norm": 0.2553191489361702,
      "acc_norm_stderr": 0.02601199293090201
    },
    "hendrycksTest-professional_law": {
      "acc": 0.28748370273794005,
      "acc_stderr": 0.011559337355708504,
      "acc_norm": 0.3050847457627119,
      "acc_norm_stderr": 0.011759939618085457
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.2757352941176471,
      "acc_stderr": 0.027146271936625162,
      "acc_norm": 0.2867647058823529,
      "acc_norm_stderr": 0.027472274473233818
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.3300653594771242,
      "acc_stderr": 0.019023726160724553,
      "acc_norm": 0.3088235294117647,
      "acc_norm_stderr": 0.01869085027359528
    },
    "hendrycksTest-public_relations": {
      "acc": 0.45454545454545453,
      "acc_stderr": 0.04769300568972743,
      "acc_norm": 0.2636363636363636,
      "acc_norm_stderr": 0.04220224692971987
    },
    "hendrycksTest-security_studies": {
      "acc": 0.42448979591836733,
      "acc_stderr": 0.03164209487942942,
      "acc_norm": 0.3306122448979592,
      "acc_norm_stderr": 0.030116426296540603
    },
    "hendrycksTest-sociology": {
      "acc": 0.4577114427860697,
      "acc_stderr": 0.035228658640995975,
      "acc_norm": 0.36318407960199006,
      "acc_norm_stderr": 0.03400598505599015
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.56,
      "acc_stderr": 0.049888765156985884,
      "acc_norm": 0.52,
      "acc_norm_stderr": 0.050211673156867795
    },
    "hendrycksTest-virology": {
      "acc": 0.3493975903614458,
      "acc_stderr": 0.0371172519074075,
      "acc_norm": 0.3493975903614458,
      "acc_norm_stderr": 0.037117251907407486
    },
    "hendrycksTest-world_religions": {
      "acc": 0.6198830409356725,
      "acc_stderr": 0.037229657413855394,
      "acc_norm": 0.5672514619883041,
      "acc_norm_stderr": 0.03799978644370608
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 0,
    "hendrycksTest-anatomy": 0,
    "hendrycksTest-astronomy": 0,
    "hendrycksTest-business_ethics": 0,
    "hendrycksTest-clinical_knowledge": 0,
    "hendrycksTest-college_biology": 0,
    "hendrycksTest-college_chemistry": 0,
    "hendrycksTest-college_computer_science": 0,
    "hendrycksTest-college_mathematics": 0,
    "hendrycksTest-college_medicine": 0,
    "hendrycksTest-college_physics": 0,
    "hendrycksTest-computer_security": 0,
    "hendrycksTest-conceptual_physics": 0,
    "hendrycksTest-econometrics": 0,
    "hendrycksTest-electrical_engineering": 0,
    "hendrycksTest-elementary_mathematics": 0,
    "hendrycksTest-formal_logic": 0,
    "hendrycksTest-global_facts": 0,
    "hendrycksTest-high_school_biology": 0,
    "hendrycksTest-high_school_chemistry": 0,
    "hendrycksTest-high_school_computer_science": 0,
    "hendrycksTest-high_school_european_history": 0,
    "hendrycksTest-high_school_geography": 0,
    "hendrycksTest-high_school_government_and_politics": 0,
    "hendrycksTest-high_school_macroeconomics": 0,
    "hendrycksTest-high_school_mathematics": 0,
    "hendrycksTest-high_school_microeconomics": 0,
    "hendrycksTest-high_school_physics": 0,
    "hendrycksTest-high_school_psychology": 0,
    "hendrycksTest-high_school_statistics": 0,
    "hendrycksTest-high_school_us_history": 0,
    "hendrycksTest-high_school_world_history": 0,
    "hendrycksTest-human_aging": 0,
    "hendrycksTest-human_sexuality": 0,
    "hendrycksTest-international_law": 0,
    "hendrycksTest-jurisprudence": 0,
    "hendrycksTest-logical_fallacies": 0,
    "hendrycksTest-machine_learning": 0,
    "hendrycksTest-management": 0,
    "hendrycksTest-marketing": 0,
    "hendrycksTest-medical_genetics": 0,
    "hendrycksTest-miscellaneous": 0,
    "hendrycksTest-moral_disputes": 0,
    "hendrycksTest-moral_scenarios": 0,
    "hendrycksTest-nutrition": 0,
    "hendrycksTest-philosophy": 0,
    "hendrycksTest-prehistory": 0,
    "hendrycksTest-professional_accounting": 0,
    "hendrycksTest-professional_law": 0,
    "hendrycksTest-professional_medicine": 0,
    "hendrycksTest-professional_psychology": 0,
    "hendrycksTest-public_relations": 0,
    "hendrycksTest-security_studies": 0,
    "hendrycksTest-sociology": 0,
    "hendrycksTest-us_foreign_policy": 0,
    "hendrycksTest-virology": 0,
    "hendrycksTest-world_religions": 0
  },
  "config": {
    "model": "gmukobi/shf-7b-accum-1",
    "model_args": null,
    "num_fewshot": 0,
    "batch_size": 16,
    "device": null,
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": null
  }
}
