program: experiments/superhf/shf_iterative_v1/run_shf_iterative.py
method: grid
metric:
  goal: maximize
  name: average_score.max
parameters:
  # language_model_name:
  #   values:
  #     - "EleutherAI/pythia-1.4b-deduped"
  #     - "Rallio67/chip_1.4B_instruct_alpha"
  #     - "lambdalabs/pythia-1.4b-deduped-synthetic-instruct"
  #     - "theblackcat102/pythia-1b-deduped-sft"
      # - "EleutherAI/gpt-neo-1.3B"
      # - "EleutherAI/gpt-neo-2.7B"
      # - "gpt2-xl"
  # reward_model_name:
  #   values:
  #     - "OpenAssistant/reward-model-deberta-v3-large-v2"
  #     - "theblackcat102/reward-model-deberta-v3-base-v2"
  #     - "OpenAssistant/reward-model-electra-large-discriminator"
  # completion_filter_top_k:
  #   values:
  #     - 1
  #     - 2
  #     - 4
  #     - 8
  # superbatch_size:
  #   values:
  #     - 16
  #     - 32
  #     - 64
  #     - 96
  #     - 128
  # prompt_dataset_names:
  #   values:
  #   - ["anthropic-red-team"]
  #   - ["anthropic-red-team", "anthropic-harmless-base"]
  #   - ["anthropic-red-team", "openai/webgpt_comparisons"]
  #   - ["anthropic-harmless-base", "openai/webgpt_comparisons"]
  inverse_loss_penalty:
    values:
      - 0.0003
      - 0.001
      - 0.003
      - 0.01
  superbatch_size:
    values:
      - 8
      - 16
      - 24
      - 32
      - 48
  # learning_rate:
  #   values:
  #     - 1.0e-5
  #     - 3.0e-6
  #     - 1.0e-6
  # max_new_tokens:
  #   values:
  #     - 64
  #     - 96
  #     - 128
  # same_prompt_per_superbatch:
  #   values:
  #     - True
  #     - False
  # length_penalty:
  #   values:
  #     - 0.1
  #     - 0.03
  #     - 0.01
  #     - 0.003
  #     - 0.001
  #     - 0.0003
  #     - 0.0001
  #     - 0.0
