language_model_name:
  desc: HuggingFace language model to use, or "mock" for testing
  value: "EleutherAI/gpt-neo-1.3B"
reward_model_name:
  desc: HuggingFace reward model to use, or "mock" for testing
  value: "OpenAssistant/reward-model-deberta-v3-large-v2"
debug_max_prompts:
  desc: Maximum number of prompts to use, or 0 for unlimited
  value: 0
learning_rate:
  desc: Learning rate for fine-tuning the language model.
  value: 3.0e-7
scheduler_name:
  desc: Name of the scheduler to use for fine-tuning the language model.
  value: "linear"
scheduler_warmup_steps:
  desc: Number of warmup steps for the scheduler.
  value: 20
inverse_loss_penalty:
  desc: Regularizes loss by C with loss' = loss + C / loss. Set to 0 to disable.
  value: 0.1
max_prompt_char_length:
  desc: Maximum number of characters to truncate prompts to
  value: 1024
temperature:
  desc: Temperature to use for completion sampling
  value: 1.0
top_p:
  desc: Top-p to use for completion sampling
  value: 0.95
superbatch_size:
  desc: Number of completions to generate with the current policy before filtering and fine-tuning.
  value: 256
completion_filter_top_k:
  desc: Top-k completions to filter and train on.
  value: 96
max_new_tokens:
  desc: Maximum new token length of language model completion.
  value: 256
max_length_rm:
  desc: Maximum token length of reward model input.
  value: 1024
minibatch_size_generating:
  desc: Size of minibatches for generating completions.
  value: 32
minibatch_size_scoring:
  desc: Size of minibatches for scoring completions.
  value: 64
minibatch_size_finetuning:
  desc: Size of minibatches for fine-tuning the language model.
  value: 4
mixed_precision:
  desc: Whether to use lower precision fp16 optimization, or other representations.
  value: "no"
conversation_prompt:
  desc: A prompt to prepend to every prompt before generation to get the model more on distribution.
  value: "A human user asks a question or says a statement, and a helpful, polite, honest, sophisticated, emotionally aware, and humble-but-knowledgeable AI assistant responds:"
repetition_penalty:
  desc: Repetition penalty to use for completion sampling. 1.0 means no penalty (logits unchanged)
  value: 1.2
no_repeat_ngram_size:
  desc: Prevents repeating of n-grams of this size during completion sampling. 0 means no constraint (logits unchanged)
  value: 0
prompt_dataset_names:
  desc: The list of datasets to use for prompts. One or both of "anthropic-red-team" and "openai/webgpt_comparisons"
  value: ["anthropic-harmless-base", "openai/webgpt_comparisons"]
  # value: ["anthropic-harmless-base", "anthropic-red-team", "openai/webgpt_comparisons"]
