model_name:
  desc: HuggingFace language model to use, or "mock" for testing
  value: "language_model_name=gmukobi/llama-ftp-24758"
reward_model_name:
  desc: HuggingFace reward model to use, or "mock" for testing. gptneo_1.3B_first_half is for training and gptneo_1.3B_second_half is for testing.
  value: "/nlp/scr/fongsu/rm_combined/gptneo_1.3B_first_half" # "OpenAssistant/reward-model-deberta-v3-large-v2" #
test_reward_model_name:
  desc: HuggingFace reward model to use for testing.
  value: "/nlp/scr/fongsu/rm_combined/gptneo_1.3B_second_half"
seed:
  desc: The random seed to use. 66 was used originally
  value: 66

log_with:
  desc: "The logger to use"
  value: "wandb"
learning_rate:
  desc: "The learning rate"
  value: 1.0e-5
mini_batch_size:
  desc: "The mini-batch size"
  value: 1
batch_size:
  desc: "The batch size" # default 256 in og code. 16 performed the best.
  value: 16
gradient_accumulation_steps:
  desc: "The number of gradient accumulation steps"
  value: 2
max_new_tokens:
  desc: "The maximum number of new tokens to generate per prompt"
  value: 64
generation_kwargs:
  desc: Extra args to pass to generation. Format is arg_name followed by value
  value: []
trim_generations_or_not:
  desc: Decides whether to trim the generated responses to exclude simulated turns of conversation
  value: true
max_prompt_char_length:
  desc: "The maximum number of characters in a prompt"
  value: 1024
lora_r:
  desc: The r dimension parameter for LoRA. Only takes effect if r and alpha are non-zero.
  value: 4 # 4
lora_alpha:
  desc: The alpha parameter for LoRA. Only takes effect if r and alpha are non-zero.
  value: 32
lora_dropout:
  desc: The dropout parameter for LoRA, or 0 to disable dropout.
  value: 0.05
lora_target_modules:
  desc: The list of modules to apply LoRA to. See src/superhf/lora.py for available modules.
  value: ["q_proj", "v_proj"]  # For LLaMa

prompt_dataset_names:
  desc: The list of datasets to use for prompts. See src/superhf/data.py for available datasets. @required.
  value: ["anthropic-red-team", "anthropic-helpful-base", "anthropic-harmless-base", "openai/webgpt_comparisons", "self_instruct"]
num_prompts:
  desc: "The maximum number of prompts to use for debugging. 0 to use all prompts."
  value: 10000
conversation_prompt:
  desc: A prompt to prepend to every prompt before generation to get the model more on distribution.
  value: "A human user sends a message, and a helpful and harmless AI assistant responds."

save_every:
    desc: "Save the model every n epochs"
    value: 64
extra_push_to_hub:
  desc: Extra intervals to push to hub
  value: [1, 4, 16, 32]
hub_repo_id:
  desc: "The ID of the HuggingFace Hub repo to save to. If empty, doesn't write to the Hub."
  value: "rlhf-fixed-feedme"

scheduler_name:
  desc: Name of the scheduler to use for fine-tuning the language model.
  value: ""
scheduler_warmup_steps:
  desc: Number of warmup steps for the scheduler.
  value: 20
init_kl_coef:
  desc: The initial KL coefficient for the KL-divergence loss. Adaptive kl is performed. Originial code sets this to 0.2
  value: 0.2
clip_kl:
  desc: Whether to clip the KL-divergence to always be greater than zero.
  value: false
normalize_reward:
  desc: Whether to normalize the reward to have mean zero
  value: false
reward_mean:
  desc: The mean of the reward distribution, if we are normalizing it.
  value: 0.0
offset_reward:
  desc: The offset of the reward distribution, to be added to all rewards.
  value: 0.0
whiten_rewards:
  desc: Whether to whiten the rewards on a per-batch basis by setting std to 1. Does not modify mean.
  value: true # Default is false
penalize_bad_endings:
  desc: Whether to set the reward for generations that don't end in the proper ending token to
  value: False
penalty_reward_value:
  desc: The reward value to use for penalizing bad endings.
  value: -1.0

top_p:
  desc: The top-p value for nucleus sampling
  value: 0.95
try_tf32_training:
  desc: Whether to try to use tf32 training
  value: true
