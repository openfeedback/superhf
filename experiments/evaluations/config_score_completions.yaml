completions_file:
  desc: Path to the file containing the completions to score
  value: "experiments/openai_generations/anthropic-harmless-base_completions_output.json"

wandb_run_id:
  desc: The wandb run id of the run to load completions from
  value: null
wandb_project_name:
  desc: The wandb project name of the run to load completions from
  value: null
wandb_entity_name:
  desc: The wandb entity name of the run to load completions from
  value: null

openai:
  desc: Whether to use the openai reward model
  value: false

language_model:
  desc: The name of the language model to use
  value: "mock"
reward_model:
  desc: The name of the reward model to use
  value: "mock"

prompt_dataset_names:
  desc: The list of datasets to use for prompts. See src/superhf/data.py for available datasets.
  value: ["anthropic-harmless-base"] # ["anthropic-red-team", "anthropic-helpful-base", "anthropic-harmless-base", "openai/webgpt_comparisons"]
max_prompt_char_length:
  desc: If generating completions, set's max length for input prompts.
  value: 1024
