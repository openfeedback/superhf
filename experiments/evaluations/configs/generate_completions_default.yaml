completions_file:
  desc: Path to the file containing the completions to score
  value: "experiments/openai_generations/anthropic-harmless-base_completions_output.json"

wandb_run_id:
  desc: The wandb run id of the run to load completions from
  value: null
wandb_project_name:
  desc: The wandb project name of the run to load completions from
  value: null
wandb_entity_name:
  desc: The wandb entity name of the run to load completions from
  value: null
use_openai:
  desc: Whether to use the openai reward model
  value: false
language_model_names:
  desc: The name of the language model to use
  value: [
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-00002",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-00004",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-00008",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-00016",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-00032",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-00064",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-00128",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-00256",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-00512",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-01024",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-01536",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-02048",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-02560",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-03072",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-03584",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-04096",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-04608",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-05120",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-05632",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-06144",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-06656",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-07680",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-08192",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-08704",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-09216",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-09728",
    "gmukobi/shf-v5-llama-gold-kl-0.35/step-10000",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-00002",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-00004",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-00008",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-00016",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-00032",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-00064",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-00128",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-00256",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-00512",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-01024",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-01536",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-02048",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-02560",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-03072",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-03584",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-04096",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-04608",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-05120",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-05632",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-06144",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-06656",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-07168",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-07680",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-08192",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-08704",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-09216",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-09728",
    "gmukobi/shf-v5-llama-gold-kl-0.0/step-10000",
  ]
language_model_interval:
  desc: The interval to use for the language model
  value: [0,0]
starting_batch_size_lm:
  desc: The starting batch size for the reward model scoring
  value: 64
max_new_tokens:
  desc: The number of new tokens to generate
  value: 64
max_prompts_per_dataset:
  desc: What subset of test prompts to use for each dataset
  value: 200
randomize_prompts_subset:
  desc: Whether to randomize the prompts subset if using a subset
  value: True

reward_model:
  desc: The name of the reward model to use
  value: "mock" # "/nlp/scr/fongsu/rm_combined/gptneo_1.3B_second_half" # "mock"
starting_batch_size_rm:
  desc: The starting batch size for the reward model scoring
  value: 64
scoring_mode:
  desc: Whether to score completions and save them or not.
  value: False

prompt_dataset_names:
  desc: The list of datasets to use for prompts. See src/superhf/data.py for available datasets.
  value: ["anthropic-red-team", "anthropic-helpful-base", "anthropic-harmless-base", "openai/webgpt_comparisons", "self_instruct"]
max_prompt_char_length:
  desc: If generating completions, set's max length for input prompts.
  value: 1024
