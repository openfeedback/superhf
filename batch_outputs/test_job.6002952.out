SLURM_JOBID=6002952
SLURM_JOB_NODELIST=jagupard35
SLURM_NNODES=1
SLURMTMPDIR=
working directory = /sailhome/fongsu/superhf
[2023-04-07 00:46:30,105] [WARNING] [runner.py:186:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2023-04-07 00:46:30,148] [INFO] [runner.py:548:main] cmd = /sailhome/fongsu/rm/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None src/reward_modelling/reward_model.py --deepspeed src/reward_modelling/ds_configs/stage_3_config.json
[2023-04-07 00:46:31,329] [INFO] [launch.py:135:main] 0 NCCL_P2P_DISABLE=1
[2023-04-07 00:46:31,330] [INFO] [launch.py:142:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2023-04-07 00:46:31,330] [INFO] [launch.py:148:main] nnodes=1, num_local_procs=4, node_rank=0
[2023-04-07 00:46:31,330] [INFO] [launch.py:161:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2023-04-07 00:46:31,330] [INFO] [launch.py:162:main] dist_world_size=4
[2023-04-07 00:46:31,330] [INFO] [launch.py:164:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
{'loss': 0.7004, 'learning_rate': 9.948905109489052e-06, 'epoch': 0.01}
{'loss': 0.6497, 'learning_rate': 9.875912408759124e-06, 'epoch': 0.01}
{'loss': 0.6087, 'learning_rate': 9.802919708029199e-06, 'epoch': 0.02}
{'loss': 0.6095, 'learning_rate': 9.729927007299271e-06, 'epoch': 0.03}
{'loss': 0.6145, 'learning_rate': 9.656934306569344e-06, 'epoch': 0.04}
{'loss': 0.6397, 'learning_rate': 9.583941605839416e-06, 'epoch': 0.04}
{'loss': 0.6383, 'learning_rate': 9.510948905109489e-06, 'epoch': 0.05}
{'loss': 0.6394, 'learning_rate': 9.437956204379563e-06, 'epoch': 0.06}
{'loss': 0.5941, 'learning_rate': 9.364963503649636e-06, 'epoch': 0.07}
{'loss': 0.5982, 'learning_rate': 9.291970802919708e-06, 'epoch': 0.07}
{'loss': 0.6212, 'learning_rate': 9.218978102189783e-06, 'epoch': 0.08}
{'loss': 0.6411, 'learning_rate': 9.145985401459855e-06, 'epoch': 0.09}
{'loss': 0.5973, 'learning_rate': 9.072992700729928e-06, 'epoch': 0.09}
{'loss': 0.623, 'learning_rate': 9e-06, 'epoch': 0.1}
{'loss': 0.6222, 'learning_rate': 8.927007299270074e-06, 'epoch': 0.11}
{'loss': 0.5818, 'learning_rate': 8.854014598540147e-06, 'epoch': 0.12}
{'loss': 0.5711, 'learning_rate': 8.78102189781022e-06, 'epoch': 0.12}
{'loss': 0.5926, 'learning_rate': 8.708029197080292e-06, 'epoch': 0.13}
{'loss': 0.6275, 'learning_rate': 8.635036496350365e-06, 'epoch': 0.14}
{'loss': 0.6076, 'learning_rate': 8.562043795620439e-06, 'epoch': 0.15}
{'eval_loss': 0.60107421875, 'eval_accuracy': 0.6758708581138487, 'eval_average_pos_score': -3.126953125, 'eval_average_neg_score': -3.505859375, 'eval_average_abs_score_diff': 0.6728515625, 'eval_runtime': 51.3291, 'eval_samples_per_second': 45.861, 'eval_steps_per_second': 1.442, 'epoch': 0.15}
{'loss': 0.5889, 'learning_rate': 8.489051094890512e-06, 'epoch': 0.15}
{'loss': 0.5985, 'learning_rate': 8.416058394160584e-06, 'epoch': 0.16}
{'loss': 0.6045, 'learning_rate': 8.343065693430657e-06, 'epoch': 0.17}
{'loss': 0.5988, 'learning_rate': 8.270072992700731e-06, 'epoch': 0.18}
{'loss': 0.6202, 'learning_rate': 8.197080291970804e-06, 'epoch': 0.18}
{'loss': 0.6074, 'learning_rate': 8.124087591240876e-06, 'epoch': 0.19}
{'loss': 0.5709, 'learning_rate': 8.05109489051095e-06, 'epoch': 0.2}
{'loss': 0.639, 'learning_rate': 7.978102189781023e-06, 'epoch': 0.2}
{'loss': 0.5801, 'learning_rate': 7.905109489051095e-06, 'epoch': 0.21}
{'loss': 0.5725, 'learning_rate': 7.83211678832117e-06, 'epoch': 0.22}
{'loss': 0.5614, 'learning_rate': 7.75912408759124e-06, 'epoch': 0.23}
{'loss': 0.5933, 'learning_rate': 7.686131386861315e-06, 'epoch': 0.23}
{'loss': 0.5675, 'learning_rate': 7.6131386861313875e-06, 'epoch': 0.24}
{'loss': 0.5811, 'learning_rate': 7.54014598540146e-06, 'epoch': 0.25}
{'loss': 0.5877, 'learning_rate': 7.4671532846715334e-06, 'epoch': 0.26}
{'loss': 0.5812, 'learning_rate': 7.394160583941606e-06, 'epoch': 0.26}
{'loss': 0.5776, 'learning_rate': 7.321167883211679e-06, 'epoch': 0.27}
{'loss': 0.6332, 'learning_rate': 7.248175182481753e-06, 'epoch': 0.28}
{'loss': 0.5662, 'learning_rate': 7.175182481751825e-06, 'epoch': 0.28}
{'loss': 0.6102, 'learning_rate': 7.102189781021899e-06, 'epoch': 0.29}
{'eval_loss': 0.58544921875, 'eval_accuracy': 0.6860662701784197, 'eval_average_pos_score': -2.869140625, 'eval_average_neg_score': -3.34765625, 'eval_average_abs_score_diff': 0.80517578125, 'eval_runtime': 51.8158, 'eval_samples_per_second': 45.43, 'eval_steps_per_second': 1.428, 'epoch': 0.29}
{'loss': 0.5924, 'learning_rate': 7.029197080291971e-06, 'epoch': 0.3}
{'loss': 0.6138, 'learning_rate': 6.956204379562045e-06, 'epoch': 0.31}
{'loss': 0.645, 'learning_rate': 6.8832116788321165e-06, 'epoch': 0.31}
{'loss': 0.5929, 'learning_rate': 6.81021897810219e-06, 'epoch': 0.32}
{'loss': 0.5812, 'learning_rate': 6.737226277372263e-06, 'epoch': 0.33}
{'loss': 0.5625, 'learning_rate': 6.664233576642336e-06, 'epoch': 0.34}
{'loss': 0.583, 'learning_rate': 6.591240875912409e-06, 'epoch': 0.34}
{'loss': 0.6319, 'learning_rate': 6.518248175182482e-06, 'epoch': 0.35}
{'loss': 0.5552, 'learning_rate': 6.445255474452555e-06, 'epoch': 0.36}
{'loss': 0.5888, 'learning_rate': 6.372262773722629e-06, 'epoch': 0.36}
{'loss': 0.6021, 'learning_rate': 6.299270072992701e-06, 'epoch': 0.37}
{'loss': 0.6126, 'learning_rate': 6.226277372262775e-06, 'epoch': 0.38}
{'loss': 0.5794, 'learning_rate': 6.153284671532847e-06, 'epoch': 0.39}
{'loss': 0.5889, 'learning_rate': 6.080291970802921e-06, 'epoch': 0.39}
{'loss': 0.5912, 'learning_rate': 6.007299270072994e-06, 'epoch': 0.4}
{'loss': 0.5644, 'learning_rate': 5.934306569343066e-06, 'epoch': 0.41}
{'loss': 0.584, 'learning_rate': 5.861313868613138e-06, 'epoch': 0.42}
{'loss': 0.5991, 'learning_rate': 5.788321167883212e-06, 'epoch': 0.42}
{'loss': 0.575, 'learning_rate': 5.715328467153285e-06, 'epoch': 0.43}
{'loss': 0.564, 'learning_rate': 5.642335766423358e-06, 'epoch': 0.44}
{'eval_loss': 0.572265625, 'eval_accuracy': 0.6949872557349193, 'eval_average_pos_score': -2.751953125, 'eval_average_neg_score': -3.380859375, 'eval_average_abs_score_diff': 0.99365234375, 'eval_runtime': 50.668, 'eval_samples_per_second': 46.459, 'eval_steps_per_second': 1.46, 'epoch': 0.44}
{'loss': 0.6028, 'learning_rate': 5.569343065693431e-06, 'epoch': 0.45}
{'loss': 0.623, 'learning_rate': 5.496350364963504e-06, 'epoch': 0.45}
{'loss': 0.5705, 'learning_rate': 5.423357664233577e-06, 'epoch': 0.46}
{'loss': 0.5672, 'learning_rate': 5.3503649635036506e-06, 'epoch': 0.47}
{'loss': 0.6024, 'learning_rate': 5.277372262773723e-06, 'epoch': 0.47}
{'loss': 0.5735, 'learning_rate': 5.2043795620437965e-06, 'epoch': 0.48}
{'loss': 0.5875, 'learning_rate': 5.13138686131387e-06, 'epoch': 0.49}
{'loss': 0.5801, 'learning_rate': 5.058394160583942e-06, 'epoch': 0.5}
{'loss': 0.565, 'learning_rate': 4.985401459854015e-06, 'epoch': 0.5}
{'loss': 0.5304, 'learning_rate': 4.9124087591240885e-06, 'epoch': 0.51}
{'loss': 0.5893, 'learning_rate': 4.839416058394161e-06, 'epoch': 0.52}
{'loss': 0.5211, 'learning_rate': 4.766423357664234e-06, 'epoch': 0.53}
{'loss': 0.5418, 'learning_rate': 4.693430656934307e-06, 'epoch': 0.53}
{'loss': 0.6317, 'learning_rate': 4.62043795620438e-06, 'epoch': 0.54}
{'loss': 0.5922, 'learning_rate': 4.547445255474453e-06, 'epoch': 0.55}
{'loss': 0.5665, 'learning_rate': 4.4744525547445264e-06, 'epoch': 0.55}
{'loss': 0.6023, 'learning_rate': 4.401459854014599e-06, 'epoch': 0.56}
{'loss': 0.5585, 'learning_rate': 4.328467153284672e-06, 'epoch': 0.57}
{'loss': 0.5836, 'learning_rate': 4.255474452554745e-06, 'epoch': 0.58}
{'loss': 0.5591, 'learning_rate': 4.1824817518248176e-06, 'epoch': 0.58}
{'eval_loss': 0.56396484375, 'eval_accuracy': 0.7013593882752761, 'eval_average_pos_score': -2.716796875, 'eval_average_neg_score': -3.345703125, 'eval_average_abs_score_diff': 0.96875, 'eval_runtime': 51.0907, 'eval_samples_per_second': 46.075, 'eval_steps_per_second': 1.448, 'epoch': 0.58}
{'loss': 0.5137, 'learning_rate': 4.109489051094891e-06, 'epoch': 0.59}
{'loss': 0.5574, 'learning_rate': 4.036496350364964e-06, 'epoch': 0.6}
{'loss': 0.5955, 'learning_rate': 3.963503649635037e-06, 'epoch': 0.61}
{'loss': 0.5549, 'learning_rate': 3.8905109489051095e-06, 'epoch': 0.61}
{'loss': 0.5913, 'learning_rate': 3.817518248175183e-06, 'epoch': 0.62}
{'loss': 0.59, 'learning_rate': 3.744525547445256e-06, 'epoch': 0.63}
{'loss': 0.577, 'learning_rate': 3.671532846715329e-06, 'epoch': 0.64}
{'loss': 0.5499, 'learning_rate': 3.598540145985402e-06, 'epoch': 0.64}
{'loss': 0.6243, 'learning_rate': 3.525547445255475e-06, 'epoch': 0.65}
{'loss': 0.5634, 'learning_rate': 3.4525547445255475e-06, 'epoch': 0.66}
{'loss': 0.5494, 'learning_rate': 3.3795620437956204e-06, 'epoch': 0.66}
{'loss': 0.5847, 'learning_rate': 3.306569343065694e-06, 'epoch': 0.67}
{'loss': 0.5528, 'learning_rate': 3.233576642335767e-06, 'epoch': 0.68}
{'loss': 0.5424, 'learning_rate': 3.16058394160584e-06, 'epoch': 0.69}
{'loss': 0.5727, 'learning_rate': 3.087591240875913e-06, 'epoch': 0.69}
{'loss': 0.5601, 'learning_rate': 3.0145985401459854e-06, 'epoch': 0.7}
{'loss': 0.5553, 'learning_rate': 2.9416058394160584e-06, 'epoch': 0.71}
{'loss': 0.6635, 'learning_rate': 2.8686131386861314e-06, 'epoch': 0.72}
{'loss': 0.6066, 'learning_rate': 2.7956204379562048e-06, 'epoch': 0.72}
{'loss': 0.5714, 'learning_rate': 2.7226277372262778e-06, 'epoch': 0.73}
{'eval_loss': 0.55908203125, 'eval_accuracy': 0.7115548003398471, 'eval_average_pos_score': -2.484375, 'eval_average_neg_score': -3.05078125, 'eval_average_abs_score_diff': 0.86572265625, 'eval_runtime': 50.7345, 'eval_samples_per_second': 46.398, 'eval_steps_per_second': 1.459, 'epoch': 0.73}
{'loss': 0.5108, 'learning_rate': 2.6496350364963508e-06, 'epoch': 0.74}
{'loss': 0.5546, 'learning_rate': 2.5766423357664233e-06, 'epoch': 0.74}
{'loss': 0.562, 'learning_rate': 2.5036496350364963e-06, 'epoch': 0.75}
{'loss': 0.5954, 'learning_rate': 2.4306569343065693e-06, 'epoch': 0.76}
{'loss': 0.5998, 'learning_rate': 2.3576642335766427e-06, 'epoch': 0.77}
{'loss': 0.5379, 'learning_rate': 2.2846715328467157e-06, 'epoch': 0.77}
{'loss': 0.5942, 'learning_rate': 2.2116788321167883e-06, 'epoch': 0.78}
{'loss': 0.5635, 'learning_rate': 2.1386861313868617e-06, 'epoch': 0.79}
{'loss': 0.5672, 'learning_rate': 2.0656934306569347e-06, 'epoch': 0.8}
{'loss': 0.514, 'learning_rate': 1.9927007299270073e-06, 'epoch': 0.8}
{'loss': 0.5417, 'learning_rate': 1.9197080291970802e-06, 'epoch': 0.81}
{'loss': 0.5985, 'learning_rate': 1.8467153284671534e-06, 'epoch': 0.82}
{'loss': 0.5314, 'learning_rate': 1.7737226277372262e-06, 'epoch': 0.82}
{'loss': 0.561, 'learning_rate': 1.7007299270072994e-06, 'epoch': 0.83}
{'loss': 0.5678, 'learning_rate': 1.6277372262773724e-06, 'epoch': 0.84}
{'loss': 0.5461, 'learning_rate': 1.5547445255474452e-06, 'epoch': 0.85}
{'loss': 0.5974, 'learning_rate': 1.4817518248175184e-06, 'epoch': 0.85}
{'loss': 0.5494, 'learning_rate': 1.4087591240875914e-06, 'epoch': 0.86}
{'loss': 0.5921, 'learning_rate': 1.3357664233576642e-06, 'epoch': 0.87}
{'loss': 0.5657, 'learning_rate': 1.2627737226277374e-06, 'epoch': 0.88}
{'eval_loss': 0.55615234375, 'eval_accuracy': 0.7022090059473237, 'eval_average_pos_score': -2.6875, 'eval_average_neg_score': -3.349609375, 'eval_average_abs_score_diff': 1.0087890625, 'eval_runtime': 50.747, 'eval_samples_per_second': 46.387, 'eval_steps_per_second': 1.458, 'epoch': 0.88}
{'loss': 0.582, 'learning_rate': 1.1897810218978104e-06, 'epoch': 0.88}
{'loss': 0.6003, 'learning_rate': 1.1167883211678833e-06, 'epoch': 0.89}
{'loss': 0.545, 'learning_rate': 1.0437956204379563e-06, 'epoch': 0.9}
{'loss': 0.5558, 'learning_rate': 9.708029197080293e-07, 'epoch': 0.91}
{'loss': 0.5624, 'learning_rate': 8.978102189781022e-07, 'epoch': 0.91}
{'loss': 0.574, 'learning_rate': 8.248175182481752e-07, 'epoch': 0.92}
{'loss': 0.5339, 'learning_rate': 7.518248175182483e-07, 'epoch': 0.93}
{'loss': 0.5737, 'learning_rate': 6.788321167883212e-07, 'epoch': 0.93}
{'loss': 0.568, 'learning_rate': 6.058394160583942e-07, 'epoch': 0.94}
{'loss': 0.5295, 'learning_rate': 5.328467153284673e-07, 'epoch': 0.95}
{'loss': 0.579, 'learning_rate': 4.5985401459854014e-07, 'epoch': 0.96}
{'loss': 0.6053, 'learning_rate': 3.8686131386861313e-07, 'epoch': 0.96}
{'loss': 0.5827, 'learning_rate': 3.138686131386862e-07, 'epoch': 0.97}
{'loss': 0.6847, 'learning_rate': 2.408759124087591e-07, 'epoch': 0.98}
{'loss': 0.5947, 'learning_rate': 1.6788321167883213e-07, 'epoch': 0.99}
{'loss': 0.5045, 'learning_rate': 9.489051094890512e-08, 'epoch': 0.99}
{'loss': 0.5542, 'learning_rate': 2.1897810218978105e-08, 'epoch': 1.0}
{'train_runtime': 4453.6203, 'train_samples_per_second': 9.843, 'train_steps_per_second': 0.308, 'train_loss': 0.584162134323677, 'epoch': 1.0}
==============END OF TRAINING===================
==============END OF TRAINING===================
==============END OF TRAINING===================
[2023-04-07 02:01:18,120] [INFO] [launch.py:350:main] Process 1784293 exits successfully.
[2023-04-07 02:01:18,121] [INFO] [launch.py:350:main] Process 1784292 exits successfully.
[2023-04-07 02:01:19,122] [INFO] [launch.py:350:main] Process 1784289 exits successfully.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[2023-04-07 02:04:48,345] [INFO] [launch.py:318:sigkill_handler] Killing subprocess 1784287
[2023-04-07 02:04:48,346] [INFO] [launch.py:318:sigkill_handler] Killing subprocess 1784289
[2023-04-07 02:04:48,346] [INFO] [launch.py:318:sigkill_handler] Killing subprocess 1784292
[2023-04-07 02:04:48,346] [INFO] [launch.py:318:sigkill_handler] Killing subprocess 1784293
[2023-04-07 02:04:48,347] [ERROR] [launch.py:324:sigkill_handler] ['/sailhome/fongsu/rm/bin/python', '-u', 'src/reward_modelling/reward_model.py', '--local_rank=3', '--deepspeed', 'src/reward_modelling/ds_configs/stage_3_config.json'] exits with return code = 1
