Using custom data configuration Anthropic--hh-rlhf-c8cd8dc58ab67414
Found cached dataset json (/sailhome/fongsu/.cache/huggingface/datasets/Anthropic___json/Anthropic--hh-rlhf-c8cd8dc58ab67414/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 171.71it/s]
Using pad_token, but it is not set yet.
Some weights of GPTNeoModel were not initialized from the model checkpoint at EleutherAI/gpt-neo-1.3B and are newly initialized: ['transformer.h.1.attn.attention.bias', 'transformer.h.17.attn.attention.bias', 'transformer.h.13.attn.attention.bias', 'transformer.h.5.attn.attention.bias', 'transformer.h.7.attn.attention.bias', 'transformer.h.3.attn.attention.bias', 'transformer.h.23.attn.attention.bias', 'transformer.h.15.attn.attention.bias', 'transformer.h.19.attn.attention.bias', 'transformer.h.9.attn.attention.bias', 'transformer.h.21.attn.attention.bias', 'transformer.h.11.attn.attention.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using custom data configuration Anthropic--hh-rlhf-b971c05e85ce84ae
Found cached dataset json (/sailhome/fongsu/.cache/huggingface/datasets/Anthropic___json/Anthropic--hh-rlhf-b971c05e85ce84ae/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 233.49it/s]
Using custom data configuration Anthropic--hh-rlhf-b971c05e85ce84ae
Found cached dataset json (/sailhome/fongsu/.cache/huggingface/datasets/Anthropic___json/Anthropic--hh-rlhf-b971c05e85ce84ae/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 615.09it/s]
/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 43835
  Num Epochs = 1
  Instantaneous batch size per device = 20
  Total train batch size (w. parallel, distributed & accumulation) = 20
  Gradient Accumulation steps = 1
  Total optimization steps = 2192
  Number of trainable parameters = 1315577856
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: oliversf (comprehelp). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.10
wandb: Run data is saved locally in /sailhome/fongsu/superhf/wandb/run-20230313_191418-m4hophzy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run derby-strudel-77
wandb: ⭐️ View project at https://wandb.ai/comprehelp/huggingface
wandb: 🚀 View run at https://wandb.ai/comprehelp/huggingface/runs/m4hophzy
  0%|          | 0/2192 [00:00<?, ?it/s]Traceback (most recent call last):
  File "src/reward_modelling/reward_model.py", line 190, in <module>
    trainer.train()
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/trainer.py", line 1543, in train
    return inner_training_loop(
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/trainer.py", line 1791, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/trainer.py", line 2539, in training_step
    loss = self.compute_loss(model, inputs)
  File "src/reward_modelling/reward_model.py", line 80, in compute_loss
    outputs = model(**inputs)
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1008, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 969, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "src/reward_modelling/reward_model.py", line 114, in forward
    return self.v_head(self.model(**inputs)[0][:,0])
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py", line 621, in forward
    outputs = block(
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py", line 327, in forward
    attn_outputs = self.attn(
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py", line 279, in forward
    return self.attention(
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py", line 242, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py", line 195, in _attn
    attn_weights = torch.where(causal_mask, attn_weights, mask_value)
RuntimeError: CUDA out of memory. Tried to allocate 906.00 MiB (GPU 0; 47.54 GiB total capacity; 44.60 GiB already allocated; 380.44 MiB free; 46.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.011 MB of 0.021 MB uploaded (0.000 MB deduped)wandb: \ 0.021 MB of 0.021 MB uploaded (0.000 MB deduped)wandb: 🚀 View run derby-strudel-77 at: https://wandb.ai/comprehelp/huggingface/runs/m4hophzy
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230313_191418-m4hophzy/logs
