[INFO|configuration_utils.py:668] 2023-04-24 21:14:25,807 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json
[INFO|configuration_utils.py:720] 2023-04-24 21:14:26,208 >> Model config DistilBertConfig {
  "_name_or_path": "distilbert-base-uncased",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.28.1",
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:14:26,217 >> loading file vocab.txt from cache at /nlp/scr/fongsu/.cache/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/vocab.txt
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:14:26,217 >> loading file tokenizer.json from cache at /nlp/scr/fongsu/.cache/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/tokenizer.json
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:14:26,217 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:14:26,217 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:14:26,217 >> loading file tokenizer_config.json from cache at /nlp/scr/fongsu/.cache/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/tokenizer_config.json
[INFO|configuration_utils.py:668] 2023-04-24 21:14:26,218 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json
[INFO|configuration_utils.py:720] 2023-04-24 21:14:26,218 >> Model config DistilBertConfig {
  "_name_or_path": "distilbert-base-uncased",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.28.1",
  "vocab_size": 30522
}

[INFO|configuration_utils.py:668] 2023-04-24 21:14:33,200 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json
[INFO|configuration_utils.py:720] 2023-04-24 21:14:33,231 >> Model config DistilBertConfig {
  "_name_or_path": "distilbert-base-uncased",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.28.1",
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:14:33,237 >> loading file vocab.txt from cache at /nlp/scr/fongsu/.cache/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/vocab.txt
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:14:33,237 >> loading file tokenizer.json from cache at /nlp/scr/fongsu/.cache/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/tokenizer.json
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:14:33,237 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:14:33,237 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:14:33,237 >> loading file tokenizer_config.json from cache at /nlp/scr/fongsu/.cache/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/tokenizer_config.json
[INFO|configuration_utils.py:668] 2023-04-24 21:14:33,237 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json
[INFO|configuration_utils.py:720] 2023-04-24 21:14:33,238 >> Model config DistilBertConfig {
  "_name_or_path": "distilbert-base-uncased",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.28.1",
  "vocab_size": 30522
}

[INFO|configuration_utils.py:668] 2023-04-24 21:14:41,218 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json
[INFO|configuration_utils.py:720] 2023-04-24 21:14:41,219 >> Model config DistilBertConfig {
  "_name_or_path": "distilbert-base-uncased",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.28.1",
  "vocab_size": 30522
}

[INFO|configuration_utils.py:668] 2023-04-24 21:14:41,999 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json
[INFO|configuration_utils.py:720] 2023-04-24 21:14:42,000 >> Model config DistilBertConfig {
  "_name_or_path": "distilbert-base-uncased",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.28.1",
  "vocab_size": 30522
}

[INFO|modeling_utils.py:2534] 2023-04-24 21:14:44,250 >> loading weights file pytorch_model.bin from cache at /nlp/scr/fongsu/.cache/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin
[WARNING|modeling_utils.py:3180] 2023-04-24 21:14:47,026 >> Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:3198] 2023-04-24 21:14:47,026 >> All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.
Traceback (most recent call last):
  File "src/reward_modelling/reward_model.py", line 285, in <module>
    data_collator=PreferenceDataCollator(tokenizer), train_dataset=training_set,
NameError: name 'training_set' is not defined
[INFO|configuration_utils.py:668] 2023-04-24 21:14:47,070 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json
[INFO|configuration_utils.py:720] 2023-04-24 21:14:47,071 >> Model config DistilBertConfig {
  "_name_or_path": "distilbert-base-uncased",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.28.1",
  "vocab_size": 30522
}

