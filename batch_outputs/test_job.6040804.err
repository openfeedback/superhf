[INFO|configuration_utils.py:668] 2023-04-24 11:33:00,379 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:668] 2023-04-24 11:33:00,379 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-24 11:33:01,684 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:720] 2023-04-24 11:33:01,686 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|tokenization_utils_base.py:1809] 2023-04-24 11:33:02,171 >> loading file vocab.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/vocab.json
[INFO|tokenization_utils_base.py:1809] 2023-04-24 11:33:02,171 >> loading file merges.txt from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/merges.txt
[INFO|tokenization_utils_base.py:1809] 2023-04-24 11:33:02,171 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-04-24 11:33:02,171 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-04-24 11:33:02,172 >> loading file special_tokens_map.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/special_tokens_map.json
[INFO|tokenization_utils_base.py:1809] 2023-04-24 11:33:02,172 >> loading file tokenizer_config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/tokenizer_config.json
[INFO|tokenization_utils_base.py:1809] 2023-04-24 11:33:02,171 >> loading file vocab.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/vocab.json
[INFO|tokenization_utils_base.py:1809] 2023-04-24 11:33:02,172 >> loading file merges.txt from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/merges.txt
[INFO|tokenization_utils_base.py:1809] 2023-04-24 11:33:02,172 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-04-24 11:33:02,172 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-04-24 11:33:02,172 >> loading file special_tokens_map.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/special_tokens_map.json
[INFO|tokenization_utils_base.py:1809] 2023-04-24 11:33:02,172 >> loading file tokenizer_config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/tokenizer_config.json
[INFO|configuration_utils.py:668] 2023-04-24 11:33:02,172 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:668] 2023-04-24 11:33:02,172 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-24 11:33:02,173 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:720] 2023-04-24 11:33:02,173 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:668] 2023-04-24 11:33:02,324 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-24 11:33:02,324 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:668] 2023-04-24 11:33:02,342 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-24 11:33:02,342 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[ERROR|tokenization_utils_base.py:1042] 2023-04-24 11:33:02,496 >> Using pad_token, but it is not set yet.
[ERROR|tokenization_utils_base.py:1042] 2023-04-24 11:33:02,499 >> Using pad_token, but it is not set yet.
[INFO|configuration_utils.py:668] 2023-04-24 11:33:02,615 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:668] 2023-04-24 11:33:02,615 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-24 11:33:02,616 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:720] 2023-04-24 11:33:02,616 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:668] 2023-04-24 11:33:02,727 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-24 11:33:02,728 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:668] 2023-04-24 11:33:02,747 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-24 11:33:02,748 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|modeling_utils.py:2534] 2023-04-24 11:33:10,876 >> loading weights file pytorch_model.bin from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/pytorch_model.bin
[INFO|modeling_utils.py:2534] 2023-04-24 11:33:10,877 >> loading weights file pytorch_model.bin from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/pytorch_model.bin
[INFO|modeling_utils.py:3190] 2023-04-24 11:34:11,671 >> All model checkpoint weights were used when initializing GPTNeoModel.

[INFO|modeling_utils.py:3198] 2023-04-24 11:34:11,672 >> All the weights of GPTNeoModel were initialized from the model checkpoint at EleutherAI/gpt-neo-1.3B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoModel for predictions without further training.
[INFO|modeling_utils.py:3190] 2023-04-24 11:34:11,725 >> All model checkpoint weights were used when initializing GPTNeoModel.

[INFO|modeling_utils.py:3198] 2023-04-24 11:34:11,725 >> All the weights of GPTNeoModel were initialized from the model checkpoint at EleutherAI/gpt-neo-1.3B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoModel for predictions without further training.
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:01<00:01,  1.46s/it] 50%|█████     | 1/2 [00:01<00:01,  1.75s/it]100%|██████████| 2/2 [00:01<00:00,  1.08it/s]
100%|██████████| 2/2 [00:01<00:00,  1.28it/s]
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:00<00:00,  5.61it/s]  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 17.87it/s]100%|██████████| 2/2 [00:00<00:00, 17.83it/s]
100%|██████████| 2/2 [00:00<00:00,  5.93it/s]100%|██████████| 2/2 [00:00<00:00,  5.87it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 15.64it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 341.64it/s]
Using /sailhome/fongsu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /sailhome/fongsu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /sailhome/fongsu/.cache/torch_extensions/py38_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module cpu_adam...
Loading extension module cpu_adam...
Using /sailhome/fongsu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /sailhome/fongsu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Emitting ninja build file /sailhome/fongsu/.cache/torch_extensions/py38_cu117/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module utils...
Loading extension module utils...
Using /sailhome/fongsu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
[WARNING|logging.py:295] 2023-04-24 11:35:12,998 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Using /sailhome/fongsu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
[INFO|trainer.py:1769] 2023-04-24 11:35:13,074 >> ***** Running training *****
[INFO|trainer.py:1770] 2023-04-24 11:35:13,074 >>   Num examples = 193,616
[INFO|trainer.py:1771] 2023-04-24 11:35:13,074 >>   Num Epochs = 1
[INFO|trainer.py:1772] 2023-04-24 11:35:13,074 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:1773] 2023-04-24 11:35:13,074 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1774] 2023-04-24 11:35:13,074 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1775] 2023-04-24 11:35:13,074 >>   Total optimization steps = 24,202
[INFO|trainer.py:1776] 2023-04-24 11:35:13,074 >>   Number of trainable parameters = 1,315,577,856
[INFO|integrations.py:720] 2023-04-24 11:35:13,075 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: oliversf (comprehelp). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.10
wandb: Run data is saved locally in /sailhome/fongsu/superhf/wandb/run-20230424_113529-tsb1uljp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prime-galaxy-270
wandb: ⭐️ View project at https://wandb.ai/comprehelp/huggingface
wandb: 🚀 View run at https://wandb.ai/comprehelp/huggingface/runs/tsb1uljp
  0%|          | 0/24202 [00:00<?, ?it/s][WARNING|logging.py:295] 2023-04-24 11:35:31,404 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
  0%|          | 1/24202 [00:06<45:46:14,  6.81s/it]  0%|          | 2/24202 [00:15<53:06:43,  7.90s/it]  0%|          | 3/24202 [00:28<70:05:09, 10.43s/it]  0%|          | 4/24202 [00:39<70:35:35, 10.50s/it]  0%|          | 5/24202 [00:49<68:55:45, 10.26s/it]  0%|          | 6/24202 [00:58<65:29:03,  9.74s/it]  0%|          | 7/24202 [01:06<63:10:32,  9.40s/it]  0%|          | 8/24202 [01:15<62:05:52,  9.24s/it]  0%|          | 9/24202 [01:24<60:58:39,  9.07s/it]  0%|          | 10/24202 [01:33<61:13:56,  9.11s/it]  0%|          | 11/24202 [01:42<59:55:53,  8.92s/it]  0%|          | 12/24202 [01:50<59:04:36,  8.79s/it]  0%|          | 13/24202 [01:59<60:01:34,  8.93s/it]  0%|          | 14/24202 [02:09<61:44:57,  9.19s/it]  0%|          | 15/24202 [02:18<60:57:07,  9.07s/it]  0%|          | 16/24202 [02:27<60:32:08,  9.01s/it]  0%|          | 17/24202 [02:37<62:02:17,  9.23s/it]  0%|          | 18/24202 [02:48<67:19:37, 10.02s/it]  0%|          | 19/24202 [02:58<65:42:16,  9.78s/it]  0%|          | 20/24202 [03:06<62:39:49,  9.33s/it]  0%|          | 21/24202 [03:14<61:09:43,  9.11s/it]  0%|          | 22/24202 [03:23<59:48:32,  8.90s/it]  0%|          | 23/24202 [03:35<65:55:46,  9.82s/it]  0%|          | 24/24202 [03:46<68:37:11, 10.22s/it]  0%|          | 25/24202 [03:56<67:49:10, 10.10s/it]  0%|          | 26/24202 [04:06<66:59:46,  9.98s/it]  0%|          | 27/24202 [04:15<66:08:50,  9.85s/it]  0%|          | 28/24202 [04:25<66:01:15,  9.83s/it]  0%|          | 29/24202 [04:36<68:08:39, 10.15s/it]  0%|          | 30/24202 [04:48<71:56:41, 10.71s/it]  0%|          | 31/24202 [04:58<70:05:39, 10.44s/it]  0%|          | 32/24202 [05:07<68:54:28, 10.26s/it]  0%|          | 33/24202 [05:17<66:32:44,  9.91s/it]  0%|          | 34/24202 [05:25<64:31:57,  9.61s/it]  0%|          | 35/24202 [05:35<64:13:44,  9.57s/it]  0%|          | 36/24202 [05:45<64:26:20,  9.60s/it]  0%|          | 37/24202 [05:55<65:30:26,  9.76s/it]  0%|          | 38/24202 [06:04<65:27:59,  9.75s/it]  0%|          | 39/24202 [06:14<64:49:03,  9.66s/it]  0%|          | 40/24202 [06:24<66:02:25,  9.84s/it]  0%|          | 41/24202 [06:34<65:10:13,  9.71s/it]  0%|          | 42/24202 [06:43<64:55:12,  9.67s/it]  0%|          | 43/24202 [06:52<62:38:13,  9.33s/it]  0%|          | 44/24202 [07:01<62:38:39,  9.34s/it]  0%|          | 45/24202 [07:11<62:56:38,  9.38s/it]  0%|          | 46/24202 [07:20<62:29:35,  9.31s/it]  0%|          | 47/24202 [07:30<63:38:05,  9.48s/it]  0%|          | 48/24202 [07:38<61:41:43,  9.20s/it]  0%|          | 49/24202 [07:47<60:55:52,  9.08s/it]  0%|          | 50/24202 [07:56<60:11:14,  8.97s/it]  0%|          | 51/24202 [08:04<58:37:48,  8.74s/it]  0%|          | 52/24202 [08:12<58:18:52,  8.69s/it]  0%|          | 53/24202 [08:21<58:14:36,  8.68s/it]  0%|          | 54/24202 [08:29<56:14:09,  8.38s/it]  0%|          | 55/24202 [08:37<56:02:48,  8.36s/it]  0%|          | 56/24202 [08:46<58:00:30,  8.65s/it]  0%|          | 57/24202 [08:55<58:28:26,  8.72s/it]  0%|          | 58/24202 [09:04<57:36:42,  8.59s/it]  0%|          | 59/24202 [09:12<57:27:31,  8.57s/it]  0%|          | 60/24202 [09:20<56:35:45,  8.44s/it]  0%|          | 61/24202 [09:29<57:42:39,  8.61s/it]  0%|          | 62/24202 [09:38<57:32:34,  8.58s/it]  0%|          | 63/24202 [09:46<57:36:43,  8.59s/it]  0%|          | 64/24202 [09:55<58:13:24,  8.68s/it]  0%|          | 65/24202 [10:04<58:23:05,  8.71s/it]  0%|          | 66/24202 [10:13<58:08:41,  8.67s/it]  0%|          | 67/24202 [10:22<60:29:55,  9.02s/it]  0%|          | 68/24202 [10:31<59:25:27,  8.86s/it]  0%|          | 69/24202 [10:40<60:45:28,  9.06s/it]  0%|          | 70/24202 [10:49<59:37:21,  8.89s/it]  0%|          | 71/24202 [10:59<62:58:32,  9.40s/it]  0%|          | 72/24202 [11:08<62:04:38,  9.26s/it]  0%|          | 73/24202 [11:17<59:40:05,  8.90s/it]  0%|          | 74/24202 [11:25<58:04:21,  8.66s/it]  0%|          | 75/24202 [11:34<59:02:00,  8.81s/it]  0%|          | 76/24202 [11:43<59:44:51,  8.92s/it]  0%|          | 77/24202 [11:51<57:40:40,  8.61s/it]  0%|          | 78/24202 [11:59<57:17:01,  8.55s/it]  0%|          | 79/24202 [12:10<61:19:41,  9.15s/it]  0%|          | 80/24202 [12:20<62:40:06,  9.35s/it]  0%|          | 81/24202 [12:29<62:17:23,  9.30s/it]  0%|          | 82/24202 [12:38<61:16:13,  9.14s/it]  0%|          | 83/24202 [12:45<58:41:43,  8.76s/it]  0%|          | 84/24202 [12:55<59:46:01,  8.92s/it]  0%|          | 85/24202 [13:03<58:37:49,  8.75s/it]  0%|          | 86/24202 [13:12<58:14:10,  8.69s/it]  0%|          | 87/24202 [13:21<59:34:36,  8.89s/it]  0%|          | 88/24202 [13:30<60:46:26,  9.07s/it]  0%|          | 89/24202 [13:39<59:09:22,  8.83s/it]  0%|          | 90/24202 [13:47<58:21:11,  8.71s/it]  0%|          | 91/24202 [13:57<59:46:44,  8.93s/it]  0%|          | 92/24202 [14:05<59:14:49,  8.85s/it]  0%|          | 93/24202 [14:14<58:47:32,  8.78s/it]  0%|          | 94/24202 [14:22<58:19:58,  8.71s/it]  0%|          | 95/24202 [14:32<59:45:53,  8.92s/it]  0%|          | 96/24202 [14:41<60:59:05,  9.11s/it]  0%|          | 97/24202 [14:50<59:01:36,  8.82s/it]  0%|          | 98/24202 [15:00<61:34:09,  9.20s/it]  0%|          | 99/24202 [15:09<61:42:26,  9.22s/it]  0%|          | 100/24202 [15:17<59:33:08,  8.90s/it]                                                        0%|          | 100/24202 [15:17<59:33:08,  8.90s/it]  0%|          | 101/24202 [15:26<59:14:59,  8.85s/it]  0%|          | 102/24202 [15:35<60:45:30,  9.08s/it]  0%|          | 103/24202 [15:44<60:04:01,  8.97s/it]  0%|          | 104/24202 [15:52<58:29:07,  8.74s/it]  0%|          | 105/24202 [16:01<58:23:30,  8.72s/it]  0%|          | 106/24202 [16:10<57:57:49,  8.66s/it]  0%|          | 107/24202 [16:18<57:40:54,  8.62s/it]  0%|          | 108/24202 [16:26<56:36:58,  8.46s/it]  0%|          | 109/24202 [16:36<59:18:12,  8.86s/it]  0%|          | 110/24202 [16:44<58:30:06,  8.74s/it]  0%|          | 111/24202 [16:53<57:42:59,  8.62s/it]  0%|          | 112/24202 [17:03<61:07:48,  9.14s/it]  0%|          | 113/24202 [17:11<58:10:52,  8.69s/it]  0%|          | 114/24202 [17:19<57:26:25,  8.58s/it]  0%|          | 115/24202 [17:28<58:25:07,  8.73s/it]slurmstepd: error: *** JOB 6040804 ON jagupard34 CANCELLED AT 2023-04-24T11:53:02 ***
slurmstepd: error: *** JOB 6040804 STEPD TERMINATED ON jagupard34 AT 2023-04-24T11:54:03 DUE TO JOB NOT ENDING WITH SIGNALS ***
