SLURM_JOBID=6037215
SLURM_JOB_NODELIST=jagupard36
SLURM_NNODES=1
SLURMTMPDIR=
working directory = /sailhome/fongsu/superhf
[2023-04-22 23:09:31,116] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2: setting --include=localhost:0,1,2
[2023-04-22 23:09:31,143] [INFO] [runner.py:540:main] cmd = /sailhome/fongsu/rm/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMl19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None src/reward_modelling/reward_model.py --model_name_or_path=EleutherAI/gpt-neo-1.3B --output_dir=/nlp/scr/fongsu/reward_model_HH/train_second_half/ --logging_steps=10 --per_device_train_batch_size=16 --per_device_eval_batch_size=16 --num_train_epochs=1 --do_eval=True --evaluation_strategy=steps --eval_steps=1000 --save_total_limit=5 --save_strategy=steps --save_steps=1000 --learning_rate=1e-5 --weight_decay=0.001 --report_to=wandb --gradient_checkpointing=True --ddp_find_unused_parameters=False --deepspeed src/reward_modelling/ds_configs/stage_3_config.json
[2023-04-22 23:09:36,561] [INFO] [launch.py:222:main] 0 NCCL_P2P_DISABLE=1
[2023-04-22 23:09:36,561] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [0, 1, 2]}
[2023-04-22 23:09:36,561] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=3, node_rank=0
[2023-04-22 23:09:36,561] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2]})
[2023-04-22 23:09:36,561] [INFO] [launch.py:247:main] dist_world_size=3
[2023-04-22 23:09:36,561] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=0,1,2
[2023-04-22 23:09:49,868] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-04-22 23:10:50,088] [INFO] [partition_parameters.py:436:__exit__] finished initializing model with 1.32B parameters
04/22/2023 23:10:57 - WARNING - datasets.builder - Found cached dataset webgpt_comparisons (/nlp/scr/fongsu/.cache/openai___webgpt_comparisons/default/0.0.0/8b5d5879cdc98c4c0099af6053dffe8d504588d43d3b11f1b1ec223ab1e8db0a)
04/22/2023 23:10:57 - WARNING - datasets.builder - Found cached dataset webgpt_comparisons (/nlp/scr/fongsu/.cache/openai___webgpt_comparisons/default/0.0.0/8b5d5879cdc98c4c0099af6053dffe8d504588d43d3b11f1b1ec223ab1e8db0a)
04/22/2023 23:10:58 - WARNING - datasets.builder - Found cached dataset webgpt_comparisons (/nlp/scr/fongsu/.cache/openai___webgpt_comparisons/default/0.0.0/8b5d5879cdc98c4c0099af6053dffe8d504588d43d3b11f1b1ec223ab1e8db0a)
04/22/2023 23:11:07 - WARNING - datasets.builder - Using custom data configuration Anthropic--hh-rlhf-c8cd8dc58ab67414
04/22/2023 23:11:07 - WARNING - datasets.builder - Using custom data configuration Anthropic--hh-rlhf-c8cd8dc58ab67414
04/22/2023 23:11:07 - WARNING - datasets.builder - Found cached dataset json (/nlp/scr/fongsu/.cache/Anthropic___json/Anthropic--hh-rlhf-c8cd8dc58ab67414/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
04/22/2023 23:11:07 - WARNING - datasets.builder - Found cached dataset json (/nlp/scr/fongsu/.cache/Anthropic___json/Anthropic--hh-rlhf-c8cd8dc58ab67414/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
04/22/2023 23:11:07 - WARNING - datasets.builder - Using custom data configuration Anthropic--hh-rlhf-c8cd8dc58ab67414
04/22/2023 23:11:07 - WARNING - datasets.builder - Found cached dataset json (/nlp/scr/fongsu/.cache/Anthropic___json/Anthropic--hh-rlhf-c8cd8dc58ab67414/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
04/22/2023 23:11:11 - WARNING - datasets.builder - Found cached dataset summarize_from_feedback (/nlp/scr/fongsu/.cache/openai___summarize_from_feedback/comparisons/0.0.0/483f970ceb55b926b0a087ef4f678ab1b089bc8174a107a452c6152e88af7ff0)
04/22/2023 23:11:11 - WARNING - datasets.builder - Found cached dataset summarize_from_feedback (/nlp/scr/fongsu/.cache/openai___summarize_from_feedback/comparisons/0.0.0/483f970ceb55b926b0a087ef4f678ab1b089bc8174a107a452c6152e88af7ff0)
04/22/2023 23:11:11 - WARNING - datasets.builder - Found cached dataset summarize_from_feedback (/nlp/scr/fongsu/.cache/openai___summarize_from_feedback/comparisons/0.0.0/483f970ceb55b926b0a087ef4f678ab1b089bc8174a107a452c6152e88af7ff0)
04/22/2023 23:11:18 - WARNING - datasets.builder - Using custom data configuration Dahoas--synthetic-instruct-gptj-pairwise-0b2fd7bd9ea121cb
04/22/2023 23:11:18 - WARNING - datasets.builder - Found cached dataset parquet (/nlp/scr/fongsu/.cache/Dahoas___parquet/Dahoas--synthetic-instruct-gptj-pairwise-0b2fd7bd9ea121cb/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
04/22/2023 23:11:18 - WARNING - datasets.builder - Using custom data configuration Dahoas--synthetic-instruct-gptj-pairwise-0b2fd7bd9ea121cb
04/22/2023 23:11:18 - WARNING - datasets.builder - Using custom data configuration Dahoas--synthetic-instruct-gptj-pairwise-0b2fd7bd9ea121cb
04/22/2023 23:11:18 - WARNING - datasets.builder - Found cached dataset parquet (/nlp/scr/fongsu/.cache/Dahoas___parquet/Dahoas--synthetic-instruct-gptj-pairwise-0b2fd7bd9ea121cb/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
04/22/2023 23:11:18 - WARNING - datasets.builder - Found cached dataset parquet (/nlp/scr/fongsu/.cache/Dahoas___parquet/Dahoas--synthetic-instruct-gptj-pairwise-0b2fd7bd9ea121cb/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
[2023-04-22 23:11:22,684] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1817898
[2023-04-22 23:11:22,685] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1817899
[2023-04-22 23:11:23,115] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1817900
[2023-04-22 23:11:23,133] [ERROR] [launch.py:434:sigkill_handler] ['/sailhome/fongsu/rm/bin/python', '-u', 'src/reward_modelling/reward_model.py', '--local_rank=2', '--model_name_or_path=EleutherAI/gpt-neo-1.3B', '--output_dir=/nlp/scr/fongsu/reward_model_HH/train_second_half/', '--logging_steps=10', '--per_device_train_batch_size=16', '--per_device_eval_batch_size=16', '--num_train_epochs=1', '--do_eval=True', '--evaluation_strategy=steps', '--eval_steps=1000', '--save_total_limit=5', '--save_strategy=steps', '--save_steps=1000', '--learning_rate=1e-5', '--weight_decay=0.001', '--report_to=wandb', '--gradient_checkpointing=True', '--ddp_find_unused_parameters=False', '--deepspeed', 'src/reward_modelling/ds_configs/stage_3_config.json'] exits with return code = 1
