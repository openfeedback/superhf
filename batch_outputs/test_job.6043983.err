[INFO|configuration_utils.py:668] 2023-04-25 17:53:02,624 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:668] 2023-04-25 17:53:02,624 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-25 17:53:02,721 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:720] 2023-04-25 17:53:02,722 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|tokenization_utils_base.py:1809] 2023-04-25 17:53:02,760 >> loading file vocab.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/vocab.json
[INFO|tokenization_utils_base.py:1809] 2023-04-25 17:53:02,760 >> loading file merges.txt from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/merges.txt
[INFO|tokenization_utils_base.py:1809] 2023-04-25 17:53:02,760 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-04-25 17:53:02,760 >> loading file vocab.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/vocab.json
[INFO|tokenization_utils_base.py:1809] 2023-04-25 17:53:02,760 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-04-25 17:53:02,760 >> loading file merges.txt from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/merges.txt
[INFO|tokenization_utils_base.py:1809] 2023-04-25 17:53:02,760 >> loading file special_tokens_map.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/special_tokens_map.json
[INFO|tokenization_utils_base.py:1809] 2023-04-25 17:53:02,760 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-04-25 17:53:02,760 >> loading file tokenizer_config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/tokenizer_config.json
[INFO|tokenization_utils_base.py:1809] 2023-04-25 17:53:02,760 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-04-25 17:53:02,760 >> loading file special_tokens_map.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/special_tokens_map.json
[INFO|tokenization_utils_base.py:1809] 2023-04-25 17:53:02,760 >> loading file tokenizer_config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/tokenizer_config.json
[INFO|configuration_utils.py:668] 2023-04-25 17:53:02,760 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:668] 2023-04-25 17:53:02,760 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-25 17:53:02,761 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:720] 2023-04-25 17:53:02,761 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:668] 2023-04-25 17:53:02,973 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-25 17:53:02,974 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:668] 2023-04-25 17:53:02,979 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-25 17:53:02,980 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[ERROR|tokenization_utils_base.py:1042] 2023-04-25 17:53:03,046 >> Using pad_token, but it is not set yet.
[ERROR|tokenization_utils_base.py:1042] 2023-04-25 17:53:03,046 >> Using pad_token, but it is not set yet.
[INFO|configuration_utils.py:668] 2023-04-25 17:53:03,178 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-25 17:53:03,179 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:668] 2023-04-25 17:53:03,183 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-25 17:53:03,185 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:668] 2023-04-25 17:53:03,331 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-25 17:53:03,332 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:668] 2023-04-25 17:53:03,410 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-25 17:53:03,411 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|modeling_utils.py:2534] 2023-04-25 17:53:03,828 >> loading weights file pytorch_model.bin from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/pytorch_model.bin
[INFO|modeling_utils.py:2534] 2023-04-25 17:53:03,828 >> loading weights file pytorch_model.bin from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/pytorch_model.bin
[INFO|modeling_utils.py:3190] 2023-04-25 17:54:00,696 >> All model checkpoint weights were used when initializing GPTNeoModel.

[INFO|modeling_utils.py:3198] 2023-04-25 17:54:00,697 >> All the weights of GPTNeoModel were initialized from the model checkpoint at EleutherAI/gpt-neo-1.3B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoModel for predictions without further training.
[INFO|modeling_utils.py:3190] 2023-04-25 17:54:00,815 >> All model checkpoint weights were used when initializing GPTNeoModel.

[INFO|modeling_utils.py:3198] 2023-04-25 17:54:00,815 >> All the weights of GPTNeoModel were initialized from the model checkpoint at EleutherAI/gpt-neo-1.3B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoModel for predictions without further training.
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:01<00:01,  1.65s/it] 50%|█████     | 1/2 [00:01<00:01,  1.62s/it]100%|██████████| 2/2 [00:01<00:00,  1.15it/s]100%|██████████| 2/2 [00:01<00:00,  1.17it/s]

  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:00<00:00,  5.85it/s]100%|██████████| 2/2 [00:00<00:00,  6.28it/s]100%|██████████| 2/2 [00:00<00:00,  6.21it/s]
100%|██████████| 2/2 [00:00<00:00,  8.37it/s]100%|██████████| 2/2 [00:00<00:00,  8.36it/s]
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 97.30it/s]100%|██████████| 1/1 [00:00<00:00, 16.79it/s]

[INFO|trainer.py:621] 2023-04-25 17:54:28,433 >> Using cuda_amp half precision backend
Using /sailhome/fongsu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /sailhome/fongsu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /sailhome/fongsu/.cache/torch_extensions/py38_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module cpu_adam...
Loading extension module cpu_adam...
Using /sailhome/fongsu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /sailhome/fongsu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Emitting ninja build file /sailhome/fongsu/.cache/torch_extensions/py38_cu117/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module utils...
Loading extension module utils...
Using /sailhome/fongsu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
[WARNING|logging.py:295] 2023-04-25 17:54:50,505 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Using /sailhome/fongsu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
[INFO|trainer.py:1769] 2023-04-25 17:54:51,329 >> ***** Running training *****
[INFO|trainer.py:1770] 2023-04-25 17:54:51,329 >>   Num examples = 193,617
[INFO|trainer.py:1771] 2023-04-25 17:54:51,329 >>   Num Epochs = 1
[INFO|trainer.py:1772] 2023-04-25 17:54:51,329 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:1773] 2023-04-25 17:54:51,330 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1774] 2023-04-25 17:54:51,330 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:1775] 2023-04-25 17:54:51,330 >>   Total optimization steps = 3,025
[INFO|trainer.py:1776] 2023-04-25 17:54:51,330 >>   Number of trainable parameters = 1,315,577,856
[INFO|integrations.py:720] 2023-04-25 17:54:51,331 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: oliversf (comprehelp). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.10
wandb: Run data is saved locally in /sailhome/fongsu/superhf/wandb/run-20230425_175455-e7w7uix0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run splendid-snowball-292
wandb: ⭐️ View project at https://wandb.ai/comprehelp/huggingface
wandb: 🚀 View run at https://wandb.ai/comprehelp/huggingface/runs/e7w7uix0
  0%|          | 0/3025 [00:00<?, ?it/s][WARNING|logging.py:295] 2023-04-25 17:54:56,029 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
  0%|          | 1/3025 [00:23<19:35:59, 23.33s/it]  0%|          | 2/3025 [00:47<19:47:01, 23.56s/it]  0%|          | 3/3025 [01:13<20:47:44, 24.77s/it]  0%|          | 4/3025 [01:38<21:05:08, 25.13s/it]  0%|          | 5/3025 [02:03<21:03:12, 25.10s/it]  0%|          | 6/3025 [02:26<20:23:18, 24.31s/it]  0%|          | 7/3025 [02:56<21:57:05, 26.18s/it]  0%|          | 8/3025 [03:22<21:54:21, 26.14s/it]  0%|          | 9/3025 [03:49<21:55:40, 26.17s/it]  0%|          | 10/3025 [04:18<22:48:43, 27.24s/it]  0%|          | 11/3025 [04:44<22:29:37, 26.87s/it]  0%|          | 12/3025 [05:09<22:04:02, 26.37s/it]  0%|          | 13/3025 [05:35<21:49:13, 26.08s/it]  0%|          | 14/3025 [06:00<21:36:26, 25.83s/it]  0%|          | 15/3025 [06:32<23:06:06, 27.63s/it]  1%|          | 16/3025 [06:59<22:53:49, 27.39s/it]  1%|          | 17/3025 [07:27<23:01:55, 27.56s/it]  1%|          | 18/3025 [07:51<22:18:04, 26.70s/it]  1%|          | 19/3025 [08:18<22:13:16, 26.61s/it]  1%|          | 20/3025 [08:44<22:11:59, 26.60s/it]  1%|          | 21/3025 [09:12<22:22:56, 26.82s/it]  1%|          | 22/3025 [09:39<22:23:15, 26.84s/it]  1%|          | 23/3025 [10:06<22:26:46, 26.92s/it]  1%|          | 24/3025 [10:31<22:04:45, 26.49s/it]  1%|          | 25/3025 [10:57<21:56:29, 26.33s/it]  1%|          | 26/3025 [11:23<21:40:55, 26.03s/it]  1%|          | 27/3025 [11:53<22:49:45, 27.41s/it]  1%|          | 28/3025 [12:18<22:13:51, 26.70s/it]  1%|          | 29/3025 [12:45<22:21:52, 26.87s/it]  1%|          | 30/3025 [13:11<22:08:16, 26.61s/it]  1%|          | 31/3025 [13:39<22:23:04, 26.92s/it]  1%|          | 32/3025 [14:04<21:58:49, 26.44s/it]  1%|          | 33/3025 [14:31<21:57:25, 26.42s/it]  1%|          | 34/3025 [14:58<22:14:06, 26.76s/it]  1%|          | 35/3025 [15:24<21:57:04, 26.43s/it]  1%|          | 36/3025 [15:50<21:49:31, 26.29s/it]  1%|          | 37/3025 [16:19<22:26:15, 27.03s/it]  1%|▏         | 38/3025 [16:45<22:08:43, 26.69s/it]  1%|▏         | 39/3025 [17:13<22:34:56, 27.23s/it]  1%|▏         | 40/3025 [17:40<22:33:19, 27.20s/it]  1%|▏         | 41/3025 [18:12<23:36:48, 28.49s/it]  1%|▏         | 42/3025 [18:38<23:05:52, 27.88s/it]  1%|▏         | 43/3025 [19:04<22:40:59, 27.38s/it]  1%|▏         | 44/3025 [19:31<22:30:46, 27.19s/it]  1%|▏         | 45/3025 [19:58<22:20:12, 26.98s/it]  2%|▏         | 46/3025 [20:24<22:05:30, 26.70s/it]  2%|▏         | 47/3025 [20:49<21:47:19, 26.34s/it]  2%|▏         | 48/3025 [21:17<22:07:47, 26.76s/it]  2%|▏         | 49/3025 [21:42<21:42:46, 26.27s/it]  2%|▏         | 50/3025 [22:08<21:38:14, 26.18s/it]  2%|▏         | 51/3025 [22:34<21:29:40, 26.02s/it]  2%|▏         | 52/3025 [23:01<21:51:27, 26.47s/it]  2%|▏         | 53/3025 [23:27<21:33:58, 26.12s/it]  2%|▏         | 54/3025 [23:55<22:01:52, 26.70s/it]  2%|▏         | 55/3025 [24:20<21:44:33, 26.35s/it]  2%|▏         | 56/3025 [24:46<21:43:24, 26.34s/it]  2%|▏         | 57/3025 [25:17<22:41:49, 27.53s/it]  2%|▏         | 58/3025 [25:44<22:34:05, 27.38s/it]  2%|▏         | 59/3025 [26:10<22:12:41, 26.96s/it]  2%|▏         | 60/3025 [26:36<21:57:47, 26.67s/it]  2%|▏         | 61/3025 [27:02<21:49:44, 26.51s/it]  2%|▏         | 62/3025 [27:28<21:49:26, 26.52s/it]  2%|▏         | 63/3025 [27:54<21:39:01, 26.31s/it]  2%|▏         | 64/3025 [28:22<22:02:23, 26.80s/it]  2%|▏         | 65/3025 [28:48<21:49:11, 26.54s/it]  2%|▏         | 66/3025 [29:21<23:18:49, 28.36s/it]  2%|▏         | 67/3025 [29:48<23:03:33, 28.06s/it]  2%|▏         | 68/3025 [30:14<22:33:47, 27.47s/it]  2%|▏         | 69/3025 [30:40<22:07:28, 26.94s/it]  2%|▏         | 70/3025 [31:06<21:57:07, 26.74s/it]  2%|▏         | 71/3025 [31:33<21:54:42, 26.70s/it]  2%|▏         | 72/3025 [31:59<21:52:05, 26.66s/it]  2%|▏         | 73/3025 [32:25<21:38:51, 26.40s/it]  2%|▏         | 74/3025 [32:52<21:52:31, 26.69s/it]  2%|▏         | 75/3025 [33:22<22:35:12, 27.56s/it]  3%|▎         | 76/3025 [33:47<21:57:21, 26.80s/it]  3%|▎         | 77/3025 [34:13<21:44:54, 26.56s/it]  3%|▎         | 78/3025 [34:43<22:31:12, 27.51s/it]  3%|▎         | 79/3025 [35:10<22:27:06, 27.44s/it]slurmstepd: error: *** JOB 6043983 ON jagupard33 CANCELLED AT 2023-04-25T18:30:08 ***
