[INFO|configuration_utils.py:668] 2023-04-24 21:25:07,779 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:668] 2023-04-24 21:25:07,779 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:668] 2023-04-24 21:25:07,779 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:668] 2023-04-24 21:25:07,779 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-24 21:25:07,886 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:720] 2023-04-24 21:25:07,887 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:720] 2023-04-24 21:25:07,889 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:720] 2023-04-24 21:25:07,889 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:25:08,344 >> loading file vocab.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/vocab.json
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:25:08,344 >> loading file vocab.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/vocab.json
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:25:08,344 >> loading file merges.txt from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/merges.txt
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:25:08,344 >> loading file merges.txt from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/merges.txt
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:25:08,344 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:25:08,344 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:25:08,344 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:25:08,344 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:25:08,344 >> loading file special_tokens_map.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/special_tokens_map.json
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:25:08,344 >> loading file special_tokens_map.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/special_tokens_map.json
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:25:08,344 >> loading file tokenizer_config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/tokenizer_config.json
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:25:08,344 >> loading file tokenizer_config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/tokenizer_config.json
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:25:08,345 >> loading file vocab.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/vocab.json
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:25:08,345 >> loading file merges.txt from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/merges.txt
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:25:08,345 >> loading file vocab.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/vocab.json
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:25:08,345 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:25:08,345 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:25:08,345 >> loading file merges.txt from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/merges.txt
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:25:08,345 >> loading file special_tokens_map.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/special_tokens_map.json
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:25:08,345 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:25:08,345 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:25:08,345 >> loading file tokenizer_config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/tokenizer_config.json
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:25:08,345 >> loading file special_tokens_map.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/special_tokens_map.json
[INFO|configuration_utils.py:668] 2023-04-24 21:25:08,345 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|tokenization_utils_base.py:1809] 2023-04-24 21:25:08,345 >> loading file tokenizer_config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/tokenizer_config.json
[INFO|configuration_utils.py:668] 2023-04-24 21:25:08,345 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:668] 2023-04-24 21:25:08,346 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:668] 2023-04-24 21:25:08,346 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-24 21:25:08,346 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:720] 2023-04-24 21:25:08,347 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:720] 2023-04-24 21:25:08,347 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:720] 2023-04-24 21:25:08,347 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:668] 2023-04-24 21:25:08,590 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-24 21:25:08,590 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:668] 2023-04-24 21:25:08,593 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:668] 2023-04-24 21:25:08,593 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-24 21:25:08,594 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:720] 2023-04-24 21:25:08,594 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:668] 2023-04-24 21:25:08,618 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-24 21:25:08,619 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[ERROR|tokenization_utils_base.py:1042] 2023-04-24 21:25:08,660 >> Using pad_token, but it is not set yet.
[ERROR|tokenization_utils_base.py:1042] 2023-04-24 21:25:08,660 >> Using pad_token, but it is not set yet.
[ERROR|tokenization_utils_base.py:1042] 2023-04-24 21:25:08,662 >> Using pad_token, but it is not set yet.
[ERROR|tokenization_utils_base.py:1042] 2023-04-24 21:25:08,667 >> Using pad_token, but it is not set yet.
[INFO|configuration_utils.py:668] 2023-04-24 21:25:08,786 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-24 21:25:08,788 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:668] 2023-04-24 21:25:08,788 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-24 21:25:08,790 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:668] 2023-04-24 21:25:08,795 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-24 21:25:08,796 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:668] 2023-04-24 21:25:08,807 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-24 21:25:08,808 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:668] 2023-04-24 21:25:08,913 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-24 21:25:08,914 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:668] 2023-04-24 21:25:08,915 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-24 21:25:08,916 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:668] 2023-04-24 21:25:08,918 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-24 21:25:08,919 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|configuration_utils.py:668] 2023-04-24 21:25:08,935 >> loading configuration file config.json from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/config.json
[INFO|configuration_utils.py:720] 2023-04-24 21:25:08,936 >> Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-1.3B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      12
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 16,
  "num_layers": 24,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

[INFO|modeling_utils.py:2534] 2023-04-24 21:25:09,491 >> loading weights file pytorch_model.bin from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/pytorch_model.bin
[INFO|modeling_utils.py:2534] 2023-04-24 21:25:09,491 >> loading weights file pytorch_model.bin from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/pytorch_model.bin
[INFO|modeling_utils.py:2534] 2023-04-24 21:25:09,491 >> loading weights file pytorch_model.bin from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/pytorch_model.bin
[INFO|modeling_utils.py:2534] 2023-04-24 21:25:09,491 >> loading weights file pytorch_model.bin from cache at /nlp/scr/fongsu/.cache/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572/pytorch_model.bin
[INFO|modeling_utils.py:2623] 2023-04-24 21:25:58,634 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2023-04-24 21:25:58,634 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2023-04-24 21:25:58,637 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2023-04-24 21:25:58,637 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:3190] 2023-04-24 21:26:15,167 >> All model checkpoint weights were used when initializing GPTNeoModel.

[INFO|modeling_utils.py:3190] 2023-04-24 21:26:15,167 >> All model checkpoint weights were used when initializing GPTNeoModel.

[INFO|modeling_utils.py:3198] 2023-04-24 21:26:15,167 >> All the weights of GPTNeoModel were initialized from the model checkpoint at EleutherAI/gpt-neo-1.3B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoModel for predictions without further training.
[INFO|modeling_utils.py:3190] 2023-04-24 21:26:15,167 >> All model checkpoint weights were used when initializing GPTNeoModel.

[INFO|modeling_utils.py:3198] 2023-04-24 21:26:15,167 >> All the weights of GPTNeoModel were initialized from the model checkpoint at EleutherAI/gpt-neo-1.3B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoModel for predictions without further training.
[INFO|modeling_utils.py:3190] 2023-04-24 21:26:15,167 >> All model checkpoint weights were used when initializing GPTNeoModel.

[INFO|modeling_utils.py:3198] 2023-04-24 21:26:15,167 >> All the weights of GPTNeoModel were initialized from the model checkpoint at EleutherAI/gpt-neo-1.3B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoModel for predictions without further training.
[INFO|modeling_utils.py:3198] 2023-04-24 21:26:15,167 >> All the weights of GPTNeoModel were initialized from the model checkpoint at EleutherAI/gpt-neo-1.3B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoModel for predictions without further training.
Using /sailhome/fongsu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /sailhome/fongsu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /sailhome/fongsu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /sailhome/fongsu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /sailhome/fongsu/.cache/torch_extensions/py38_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module cpu_adam...
Loading extension module cpu_adam...
Loading extension module cpu_adam...
Loading extension module cpu_adam...
Using /sailhome/fongsu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /sailhome/fongsu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /sailhome/fongsu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /sailhome/fongsu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Emitting ninja build file /sailhome/fongsu/.cache/torch_extensions/py38_cu117/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Using /sailhome/fongsu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /sailhome/fongsu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /sailhome/fongsu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Using /sailhome/fongsu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
[INFO|trainer.py:1769] 2023-04-24 21:26:52,373 >> ***** Running training *****
[INFO|trainer.py:1770] 2023-04-24 21:26:52,373 >>   Num examples = 14,346
[INFO|trainer.py:1771] 2023-04-24 21:26:52,373 >>   Num Epochs = 1
[INFO|trainer.py:1772] 2023-04-24 21:26:52,373 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:1773] 2023-04-24 21:26:52,374 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1774] 2023-04-24 21:26:52,374 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1775] 2023-04-24 21:26:52,374 >>   Total optimization steps = 224
[INFO|trainer.py:1776] 2023-04-24 21:26:52,375 >>   Number of trainable parameters = 1,315,577,856
[INFO|integrations.py:720] 2023-04-24 21:26:52,376 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: oliversf (comprehelp). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.10
wandb: Run data is saved locally in /sailhome/fongsu/superhf/wandb/run-20230424_212656-zegpwhaj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-salad-277
wandb: ‚≠êÔ∏è View project at https://wandb.ai/comprehelp/huggingface
wandb: üöÄ View run at https://wandb.ai/comprehelp/huggingface/runs/zegpwhaj
  0%|          | 0/224 [00:00<?, ?it/s][WARNING|logging.py:295] 2023-04-24 21:26:57,077 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[WARNING|logging.py:295] 2023-04-24 21:26:57,077 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[WARNING|logging.py:295] 2023-04-24 21:26:57,077 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[WARNING|logging.py:295] 2023-04-24 21:26:57,077 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
  0%|          | 1/224 [00:19<1:11:37, 19.27s/it]  1%|          | 2/224 [00:35<1:05:27, 17.69s/it]  1%|‚ñè         | 3/224 [00:52<1:02:34, 16.99s/it]  2%|‚ñè         | 4/224 [01:08<1:02:13, 16.97s/it]  2%|‚ñè         | 5/224 [01:26<1:02:27, 17.11s/it]  3%|‚ñé         | 6/224 [01:42<1:01:37, 16.96s/it]  3%|‚ñé         | 7/224 [01:59<1:00:21, 16.69s/it]  4%|‚ñé         | 8/224 [02:16<1:00:29, 16.80s/it]  4%|‚ñç         | 9/224 [02:32<59:58, 16.74s/it]    4%|‚ñç         | 10/224 [02:49<59:17, 16.62s/it]  5%|‚ñç         | 11/224 [03:05<58:58, 16.61s/it]  5%|‚ñå         | 12/224 [03:22<58:31, 16.56s/it]  6%|‚ñå         | 13/224 [03:38<58:23, 16.60s/it]  6%|‚ñã         | 14/224 [03:56<58:55, 16.83s/it]  7%|‚ñã         | 15/224 [04:13<59:27, 17.07s/it]  7%|‚ñã         | 16/224 [04:31<59:49, 17.26s/it]  8%|‚ñä         | 17/224 [04:48<58:45, 17.03s/it]  8%|‚ñä         | 18/224 [05:05<58:25, 17.02s/it]  8%|‚ñä         | 19/224 [05:21<57:44, 16.90s/it]  9%|‚ñâ         | 20/224 [05:38<57:49, 17.01s/it]  9%|‚ñâ         | 21/224 [05:56<58:34, 17.31s/it] 10%|‚ñâ         | 22/224 [06:14<58:40, 17.43s/it] 10%|‚ñà         | 23/224 [06:31<57:24, 17.14s/it] 11%|‚ñà         | 24/224 [06:48<56:55, 17.08s/it] 11%|‚ñà         | 25/224 [07:04<56:15, 16.96s/it] 12%|‚ñà‚ñè        | 26/224 [07:21<55:36, 16.85s/it] 12%|‚ñà‚ñè        | 27/224 [07:38<55:39, 16.95s/it] 12%|‚ñà‚ñé        | 28/224 [07:55<55:05, 16.87s/it] 13%|‚ñà‚ñé        | 29/224 [08:11<54:02, 16.63s/it] 13%|‚ñà‚ñé        | 30/224 [08:28<53:56, 16.68s/it] 14%|‚ñà‚ñç        | 31/224 [08:45<54:47, 17.04s/it] 14%|‚ñà‚ñç        | 32/224 [09:02<53:48, 16.82s/it] 15%|‚ñà‚ñç        | 33/224 [09:19<53:37, 16.85s/it] 15%|‚ñà‚ñå        | 34/224 [09:40<58:01, 18.32s/it] 16%|‚ñà‚ñå        | 35/224 [09:57<56:22, 17.90s/it] 16%|‚ñà‚ñå        | 36/224 [10:14<55:07, 17.59s/it] 17%|‚ñà‚ñã        | 37/224 [10:31<53:49, 17.27s/it] 17%|‚ñà‚ñã        | 38/224 [10:47<52:43, 17.01s/it] 17%|‚ñà‚ñã        | 39/224 [11:04<51:56, 16.84s/it] 18%|‚ñà‚ñä        | 40/224 [11:21<51:52, 16.92s/it] 18%|‚ñà‚ñä        | 41/224 [11:37<51:32, 16.90s/it] 19%|‚ñà‚ñâ        | 42/224 [11:54<50:55, 16.79s/it] 19%|‚ñà‚ñâ        | 43/224 [12:14<53:11, 17.64s/it] 20%|‚ñà‚ñâ        | 44/224 [12:31<52:46, 17.59s/it] 20%|‚ñà‚ñà        | 45/224 [12:47<51:12, 17.17s/it] 21%|‚ñà‚ñà        | 46/224 [13:03<50:03, 16.87s/it] 21%|‚ñà‚ñà        | 47/224 [13:22<50:55, 17.26s/it] 21%|‚ñà‚ñà‚ñè       | 48/224 [13:38<50:12, 17.12s/it] 22%|‚ñà‚ñà‚ñè       | 49/224 [13:55<49:42, 17.04s/it] 22%|‚ñà‚ñà‚ñè       | 50/224 [14:12<49:11, 16.96s/it] 23%|‚ñà‚ñà‚ñé       | 51/224 [14:30<49:27, 17.15s/it] 23%|‚ñà‚ñà‚ñé       | 52/224 [14:47<49:32, 17.28s/it] 24%|‚ñà‚ñà‚ñé       | 53/224 [15:08<52:01, 18.25s/it] 24%|‚ñà‚ñà‚ñç       | 54/224 [15:24<50:02, 17.66s/it] 25%|‚ñà‚ñà‚ñç       | 55/224 [15:41<49:30, 17.58s/it] 25%|‚ñà‚ñà‚ñå       | 56/224 [15:58<48:35, 17.36s/it] 25%|‚ñà‚ñà‚ñå       | 57/224 [16:16<48:45, 17.52s/it] 26%|‚ñà‚ñà‚ñå       | 58/224 [16:32<47:27, 17.15s/it] 26%|‚ñà‚ñà‚ñã       | 59/224 [16:49<46:55, 17.06s/it] 27%|‚ñà‚ñà‚ñã       | 60/224 [17:06<46:17, 16.94s/it] 27%|‚ñà‚ñà‚ñã       | 61/224 [17:22<45:37, 16.80s/it] 28%|‚ñà‚ñà‚ñä       | 62/224 [17:39<45:16, 16.77s/it] 28%|‚ñà‚ñà‚ñä       | 63/224 [17:56<45:10, 16.84s/it] 29%|‚ñà‚ñà‚ñä       | 64/224 [18:12<44:28, 16.68s/it] 29%|‚ñà‚ñà‚ñâ       | 65/224 [18:29<44:21, 16.74s/it] 29%|‚ñà‚ñà‚ñâ       | 66/224 [18:46<43:50, 16.65s/it] 30%|‚ñà‚ñà‚ñâ       | 67/224 [19:02<43:27, 16.61s/it] 30%|‚ñà‚ñà‚ñà       | 68/224 [19:19<43:12, 16.62s/it] 31%|‚ñà‚ñà‚ñà       | 69/224 [19:35<42:44, 16.55s/it] 31%|‚ñà‚ñà‚ñà‚ñè      | 70/224 [19:53<43:06, 16.79s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 71/224 [20:09<42:48, 16.79s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 72/224 [20:27<42:54, 16.94s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 73/224 [20:43<42:21, 16.83s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 74/224 [21:00<41:43, 16.69s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 75/224 [21:17<42:06, 16.96s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 76/224 [21:34<41:30, 16.83s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 77/224 [21:50<40:47, 16.65s/it] 35%|‚ñà‚ñà‚ñà‚ñç      | 78/224 [22:07<40:30, 16.65s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 79/224 [22:23<40:07, 16.60s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 80/224 [22:39<39:38, 16.52s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 81/224 [22:56<39:37, 16.63s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 82/224 [23:13<39:13, 16.57s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 83/224 [23:29<38:51, 16.54s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 84/224 [23:46<38:23, 16.46s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 85/224 [24:02<38:27, 16.60s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 86/224 [24:19<37:54, 16.48s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 87/224 [24:36<38:10, 16.72s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 88/224 [24:52<37:32, 16.56s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 89/224 [25:09<37:46, 16.79s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 90/224 [25:26<37:04, 16.60s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 91/224 [25:42<36:54, 16.65s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 92/224 [26:01<37:41, 17.13s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 93/224 [26:19<38:00, 17.41s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 94/224 [26:36<37:39, 17.38s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 95/224 [26:54<37:29, 17.44s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 96/224 [27:10<36:32, 17.13s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 97/224 [27:28<36:34, 17.28s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 98/224 [27:45<36:24, 17.34s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 99/224 [28:01<35:27, 17.02s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 100/224 [28:18<35:04, 16.97s/it]Traceback (most recent call last):
  File "src/reward_modelling/reward_model.py", line 293, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
  File "src/reward_modelling/reward_model.py", line 293, in <module>
  File "src/reward_modelling/reward_model.py", line 293, in <module>
                                                  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 100/224 [28:18<35:04, 16.97s/it]    result = trainer.train()
      File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/trainer.py", line 1662, in train
result = trainer.train()
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/trainer.py", line 1662, in train
    result = trainer.train()
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/trainer.py", line 1662, in train
        return inner_training_loop(return inner_training_loop(

  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/trainer.py", line 2006, in _inner_training_loop
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/trainer.py", line 2006, in _inner_training_loop
    return inner_training_loop(
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/trainer.py", line 2006, in _inner_training_loop
Traceback (most recent call last):
  File "src/reward_modelling/reward_model.py", line 293, in <module>
    result = trainer.train()
        self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)

  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/trainer.py", line 2287, in _maybe_log_save_evaluate
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/trainer.py", line 2287, in _maybe_log_save_evaluate
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/trainer.py", line 2006, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/trainer.py", line 2287, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
        metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)

  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/trainer.py", line 2989, in evaluate
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/trainer.py", line 2989, in evaluate
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/trainer.py", line 2989, in evaluate
    eval_dataloader = self.get_eval_dataloader(eval_dataset)
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/trainer.py", line 959, in get_eval_dataloader
    raise ValueError("Trainer: evaluation requires an eval_dataset.")
ValueError: Trainer: evaluation requires an eval_dataset.
    eval_dataloader = self.get_eval_dataloader(eval_dataset)
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/trainer.py", line 959, in get_eval_dataloader
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
    raise ValueError("Trainer: evaluation requires an eval_dataset.")  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/trainer.py", line 2287, in _maybe_log_save_evaluate

ValueError: Trainer: evaluation requires an eval_dataset.
    eval_dataloader = self.get_eval_dataloader(eval_dataset)
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/trainer.py", line 959, in get_eval_dataloader
    raise ValueError("Trainer: evaluation requires an eval_dataset.")
ValueError: Trainer: evaluation requires an eval_dataset.
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/trainer.py", line 2989, in evaluate
    eval_dataloader = self.get_eval_dataloader(eval_dataset)
  File "/sailhome/fongsu/rm/lib/python3.8/site-packages/transformers/trainer.py", line 959, in get_eval_dataloader
    raise ValueError("Trainer: evaluation requires an eval_dataset.")
ValueError: Trainer: evaluation requires an eval_dataset.
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: \ 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: | 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)